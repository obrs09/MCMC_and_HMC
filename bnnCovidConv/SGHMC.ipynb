{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9923264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndim\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import argparse\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import collections\n",
    "import h5py, sys\n",
    "import gzip\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import matplotlib\n",
    "\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from numpy.random import gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecda9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_resize_size = 256\n",
    "image_trans_size = 224\n",
    "batch_size = 50\n",
    "nb_epochs = 100\n",
    "\n",
    "pSGLD = False\n",
    "save_data = True\n",
    "n_samples = 90\n",
    "\n",
    "sample_freq = 2\n",
    "burn_in = 500\n",
    "\n",
    "\n",
    "\n",
    "prior_sig = 0.1\n",
    "# Where to save models weights\n",
    "models_dir = 'models_SGHMC_COVID150'\n",
    "# Where to save plots and error, accuracy vectors\n",
    "results_dir = 'results_SGHMC_COVID150'\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "model= 'Gaussian_prior'\n",
    "nsamples = int(n_samples)\n",
    "\n",
    "\n",
    "## weight saving parameters #######\n",
    "\n",
    "\n",
    "N_saves = 100\n",
    "resample_its = 50\n",
    "resample_prior_its = 15\n",
    "re_burn = 1e8\n",
    "###################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## weight saving parameters #######\n",
    "save_num = 10\n",
    "sim_steps = int(nb_epochs/save_num)  \n",
    "\n",
    "###################################\n",
    "\n",
    "Nsamples = nsamples\n",
    "\n",
    "save_every = int(nb_epochs/20)  \n",
    "# We sample every 2 epochs as I have found samples to be correlated after only 1\n",
    "num_workers = 4\n",
    "nhid = 1200\n",
    "\n",
    "grad_std_mul=20\n",
    "\n",
    "# transform_covid19 = transforms.Compose([\n",
    "#     transforms.Resize(image_trans_size),\n",
    "#     transforms.CenterCrop(image_trans_size),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     transforms.Grayscale(num_output_channels=1)\n",
    "# ])\n",
    "\n",
    "transform_covid19 = transforms.Compose([\n",
    "    transforms.Resize(image_resize_size),\n",
    "    transforms.CenterCrop(image_trans_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b5e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d20a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "\n",
    "\n",
    "def humansize(nbytes):\n",
    "    i = 0\n",
    "    while nbytes >= 1024 and i < len(suffixes) - 1:\n",
    "        nbytes /= 1024.\n",
    "        i += 1\n",
    "    f = ('%.2f' % nbytes)\n",
    "    return '%s%s' % (f, suffixes[i])\n",
    "\n",
    "\n",
    "def get_num_batches(nb_samples, batch_size, roundup=True):\n",
    "    if roundup:\n",
    "        return ((nb_samples + (-nb_samples % batch_size)) / batch_size)  # roundup division\n",
    "    else:\n",
    "        return nb_samples / batch_size\n",
    "\n",
    "\n",
    "def generate_ind_batch(nb_samples, batch_size, random=True, roundup=True):\n",
    "    if random:\n",
    "        ind = np.random.permutation(nb_samples)\n",
    "    else:\n",
    "        ind = range(int(nb_samples))\n",
    "    for i in range(int(get_num_batches(nb_samples, batch_size, roundup))):\n",
    "        yield ind[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def shuffle_in_unison_scary(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "class DatafeedImage(data.Dataset):\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "\n",
    "### functions for BNN with gauss output: ###\n",
    "\n",
    "def diagonal_gauss_loglike(x, mu, sigma):\n",
    "    # note that we can just treat each dim as isotropic and then do sum\n",
    "    cte_term = -(0.5)*np.log(2*np.pi)\n",
    "    det_sig_term = -torch.log(sigma)\n",
    "    inner = (x - mu)/sigma\n",
    "    dist_term = -(0.5)*(inner**2)\n",
    "    log_px = (cte_term + det_sig_term + dist_term).sum(dim=1, keepdim=False)\n",
    "    return log_px\n",
    "\n",
    "def get_rms(mu, y, y_means, y_stds):\n",
    "    x_un = mu * y_stds + y_means\n",
    "    y_un = y * y_stds + y_means\n",
    "    return torch.sqrt(((x_un - y_un)**2).sum() / y.shape[0])\n",
    "\n",
    "\n",
    "def get_loglike(mu, sigma, y, y_means, y_stds):\n",
    "    mu_un = mu * y_stds + y_means\n",
    "    y_un = y * y_stds + y_means\n",
    "    sigma_un = sigma * y_stds\n",
    "    ll = diagonal_gauss_loglike(y_un, mu_un, sigma_un)\n",
    "    return ll.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0584ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e8b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class H_SA_SGHMC(Optimizer):\n",
    "    \"\"\" Stochastic Gradient Hamiltonian Monte-Carlo Sampler that uses scale adaption during burn-in\n",
    "        procedure to find some hyperparamters. A gaussian prior is placed over parameters and a Gamma\n",
    "        Hyperprior is placed over the prior's standard deviation\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-2, base_C=0.05, gauss_sig=0.1, alpha0=10, beta0=10):\n",
    "\n",
    "        self.eps = 1e-6\n",
    "        self.alpha0 = alpha0\n",
    "        self.beta0 = beta0\n",
    "\n",
    "        if gauss_sig == 0:\n",
    "            self.weight_decay = 0\n",
    "        else:\n",
    "            self.weight_decay = 1 / (gauss_sig ** 2)\n",
    "\n",
    "        if self.weight_decay <= 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if base_C < 0:\n",
    "            raise ValueError(\"Invalid friction term: {}\".format(base_C))\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            base_C=base_C,\n",
    "        )\n",
    "        super(H_SA_SGHMC, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        \"\"\"Simulate discretized Hamiltonian dynamics for one step\"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:  # iterate over blocks -> the ones defined in defaults. We dont use groups.\n",
    "            for p in group[\"params\"]:  # these are weight and bias matrices\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]  # define dict for each individual param\n",
    "                if len(state) == 0:\n",
    "                    state[\"iteration\"] = 0\n",
    "                    state[\"tau\"] = torch.ones_like(p)\n",
    "                    state[\"g\"] = torch.ones_like(p)\n",
    "                    state[\"V_hat\"] = torch.ones_like(p)\n",
    "                    state[\"v_momentum\"] = torch.zeros_like(\n",
    "                        p)  # p.data.new(p.data.size()).normal_(mean=0, std=np.sqrt(group[\"lr\"])) #\n",
    "                    state['weight_decay'] = self.weight_decay\n",
    "\n",
    "                state[\"iteration\"] += 1  # this is kind of useless now but lets keep it provisionally\n",
    "\n",
    "                if resample_prior:\n",
    "                    alpha = self.alpha0 + p.data.nelement() / 2\n",
    "                    beta = self.beta0 + (p.data ** 2).sum().item() / 2\n",
    "                    gamma_sample = gamma(shape=alpha, scale=1 / (beta), size=None)\n",
    "                    #                     print('std', 1/np.sqrt(gamma_sample))\n",
    "                    state['weight_decay'] = gamma_sample\n",
    "\n",
    "                base_C, lr = group[\"base_C\"], group[\"lr\"]\n",
    "                weight_decay = state[\"weight_decay\"]\n",
    "                tau, g, V_hat = state[\"tau\"], state[\"g\"], state[\"V_hat\"]\n",
    "\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                # update parameters during burn-in\n",
    "                if burn_in:  # We update g first as it makes most sense\n",
    "                    tau.add_(-tau * (g ** 2) / (\n",
    "                                V_hat + self.eps) + 1)  # specifies the moving average window, see Eq 9 in [1] left\n",
    "                    tau_inv = 1. / (tau + self.eps)\n",
    "                    g.add_(-tau_inv * g + tau_inv * d_p)  # average gradient see Eq 9 in [1] right\n",
    "                    V_hat.add_(-tau_inv * V_hat + tau_inv * (d_p ** 2))  # gradient variance see Eq 8 in [1]\n",
    "\n",
    "                V_sqrt = torch.sqrt(V_hat)\n",
    "                V_inv_sqrt = 1. / (V_sqrt + self.eps)  # preconditioner\n",
    "\n",
    "                if resample_momentum:  # equivalent to var = M under momentum reparametrisation\n",
    "                    state[\"v_momentum\"] = torch.normal(mean=torch.zeros_like(d_p),\n",
    "                                                       std=torch.sqrt((lr ** 2) * V_inv_sqrt))\n",
    "                v_momentum = state[\"v_momentum\"]\n",
    "\n",
    "                noise_var = (2. * (lr ** 2) * V_inv_sqrt * base_C - (lr ** 4))\n",
    "                noise_std = torch.sqrt(torch.clamp(noise_var, min=1e-16))\n",
    "                # sample random epsilon\n",
    "                noise_sample = torch.normal(mean=torch.zeros_like(d_p), std=torch.ones_like(d_p) * noise_std)\n",
    "\n",
    "                # update momentum (Eq 10 right in [1])\n",
    "                v_momentum.add_(- (lr ** 2) * V_inv_sqrt * d_p - base_C * v_momentum + noise_sample)\n",
    "\n",
    "                # update theta (Eq 10 left in [1])\n",
    "                p.data.add_(v_momentum)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef19b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "\n",
    "        layers = [nn.Linear(input_dim, width), nn.ReLU()]\n",
    "        for i in range(depth - 1):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(width, output_dim))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Linear_2L(nn.Module):\n",
    "    \"\"\"\n",
    "    To train on CIFAR-10:\n",
    "    https://arxiv.org/pdf/1207.0580.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, outputs, side_in):\n",
    "        super(Linear_2L, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(inputs, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class BNN_cat(BaseNet):  # for categorical distributions\n",
    "    def __init__(self, channels_in=3, side_in=224, classes=2, N_train = 300, lr=1e-2, cuda=True, grad_std_mul=30):\n",
    "        super(BNN_cat, self).__init__()\n",
    "\n",
    "        cprint('y', 'BNN categorical output')\n",
    "        self.lr = lr\n",
    "        self.channels_in = channels_in\n",
    "        self.side_in = side_in\n",
    "        self.classes = classes\n",
    "#         self.model = MLP(input_dim=self.channels_in * self.side_in * self.side_in, \n",
    "#                          width=1200, depth=2, output_dim=self.classes)\n",
    "        self.model = Linear_2L(inputs=self.channels_in, outputs=self.classes, side_in=self.side_in)\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.N_train = N_train\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.grad_buff = []\n",
    "        self.max_grad = 1e20\n",
    "        self.grad_std_mul = grad_std_mul\n",
    "\n",
    "        self.weight_set_samples = []\n",
    "\n",
    "    def create_net(self):\n",
    "#         torch.manual_seed(42)\n",
    "#         if self.cuda:\n",
    "#             torch.cuda.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        \"\"\"This optimiser incorporates the gaussian prior term automatically. The prior variance is gibbs sampled from\n",
    "        its posterior using a gamma hyper-prior.\"\"\"\n",
    "        self.optimizer = H_SA_SGHMC(params=self.model.parameters(), lr=self.lr, base_C=0.05, gauss_sig=0.1)  # this last parameter does nothing\n",
    "\n",
    "    def fit(self, x, y, burn_in=False, resample_momentum=False, resample_prior=False):\n",
    "        self.set_mode_train(train=True)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='mean')\n",
    "        loss = loss * self.N_train  # We use mean because we treat as an estimation of whole dataset\n",
    "        loss.backward()\n",
    "        #print('len', len(self.grad_buff))\n",
    "        #print([self.grad_buff[i].cpu() for i in range(len(self.grad_buff))])\n",
    "        # Gradient buffer to allow for dynamic clipping and prevent explosions\n",
    "        if len(self.grad_buff) > 1000:\n",
    "            #self.grad_buff = [self.grad_buff[i].cpu() for i in range(len(self.grad_buff))]\n",
    "            #print('len',len(self.grad_buff), self.grad_buff)\n",
    "            #print(type(self.grad_buff))\n",
    "            #print(np.array(self.grad_buff))\n",
    "\n",
    "            self.max_grad = np.mean([self.grad_buff[i].cpu() for i in range(len(self.grad_buff))]) + self.grad_std_mul * np.std([self.grad_buff[i].cpu() for i in range(len(self.grad_buff))])\n",
    "            self.grad_buff.pop(0)\n",
    "        # Clipping to prevent explosions\n",
    "        self.grad_buff.append(nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n",
    "                                                       max_norm=self.max_grad, norm_type=2))\n",
    "        if self.grad_buff[-1] >= self.max_grad:\n",
    "            print(self.max_grad, self.grad_buff[-1])\n",
    "            self.grad_buff.pop()\n",
    "        self.optimizer.step(burn_in=burn_in, resample_momentum=resample_momentum, resample_prior=resample_prior)\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data * x.shape[0] / self.N_train, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def save_sampled_net(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
    "\n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n",
    "        return None\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.set_mode_train(train=False)\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "        out = self.model(x)\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "        return probs.data\n",
    "\n",
    "    def sample_predict(self, x, Nsamples=0, grad=False):\n",
    "        \"\"\"return predictions using multiple samples from posterior\"\"\"\n",
    "        self.set_mode_train(train=False)\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "        x, = to_variable(var=(x, ), cuda=self.cuda)\n",
    "\n",
    "        if grad:\n",
    "            self.optimizer.zero_grad()\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        out = out[:idx]\n",
    "        prob_out = F.softmax(out, dim=2)\n",
    "\n",
    "        if grad:\n",
    "            return prob_out\n",
    "        else:\n",
    "            return prob_out.data\n",
    "\n",
    "    def sample_eval(self, x, y, Nsamples=0, grad=False):\n",
    "        #print('in sample_eval')\n",
    "        \"\"\"return predictions using multiple samples from posterior\"\"\"\n",
    "        self.set_mode_train(train=False)\n",
    "        if Nsamples == 0:\n",
    "            #print('Nsamples=0')\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        \n",
    "        \n",
    "        if grad:\n",
    "            print('grad')\n",
    "            self.optimizer.zero_grad()\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad = True\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "        #print('momomomomo', len(self.weight_set_samples))\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            #print('ccccccc', self.model, x.size()[0])\n",
    "            #print('mmmmmmm', x.resize_(x.size()[0], self.side_in * self.side_in).size())\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        #print('idxxxxxxxxxx', idx)\n",
    "        out = out[:idx]\n",
    "\n",
    "        mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "\n",
    "        loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
    "        prob_out = F.softmax(mean_out, dim=1).data.cpu()\n",
    "\n",
    "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        if grad:\n",
    "            return loss.data, err, prob_out\n",
    "        else:\n",
    "            return loss.data, err, prob_out\n",
    "\n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        \"\"\"return weight samples from posterior in a single-column array\"\"\"\n",
    "        weight_vec = []\n",
    "\n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "\n",
    "            for key in state_dict.keys():\n",
    "                if 'weight' in key:\n",
    "                    weight_mtx = state_dict[key].cpu().data\n",
    "                    for weight in weight_mtx.view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "\n",
    "        return np.array(weight_vec)\n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        save_object(self.weight_set_samples, filename)\n",
    "\n",
    "    def load_weights(self, filename, subsample=1):\n",
    "        self.weight_set_samples = load_object(filename)\n",
    "        self.weight_set_samples = self.weight_set_samples[::subsample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5790c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f17b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80712553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33mBNN categorical output\u001b[0m\n",
      "    Total params: 57.01M\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_19124/272664775.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/test\", transform=transform_covid19)\n",
    "\n",
    "\n",
    "channels_in = trainset[0][0].size()[0]\n",
    "classes = np.shape(np.unique(trainset.targets))[0]\n",
    "\n",
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "NTrainPoints = train_data_len\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "nb_its_dev = log_interval\n",
    "# flat_ims = True\n",
    "flat_ims = False\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=num_workers)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=num_workers)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "net = BNN_cat(channels_in = channels_in, side_in = image_trans_size, classes = classes, \n",
    "              N_train = NTrainPoints, lr = lr, cuda = use_cuda, grad_std_mul = grad_std_mul)\n",
    "\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# net dims\n",
    "epoch = 0\n",
    "it_count = 0\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_cost = np.inf\n",
    "best_err = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e19a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_19124/1161784216.py:63: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  d_p.add_(weight_decay, p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/100, Jtr_pred = 0.693656, err = 0.512500, \u001b[31m   time: 63.756342 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 1/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.695108, err = 0.500000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/100, Jtr_pred = 0.697054, err = 0.496500, \u001b[31m   time: 65.122783 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.697175, err = 0.500000\n",
      "\u001b[0m\n",
      "it 2/100, Jtr_pred = 0.695616, err = 0.487500, \u001b[31m   time: 61.486273 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.692927, err = 0.483333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/100, Jtr_pred = 0.702468, err = 0.478500, \u001b[31m   time: 62.235911 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.688464, err = 0.433333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 4/100, Jtr_pred = 0.710029, err = 0.491500, \u001b[31m   time: 61.731115 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.691531, err = 0.473333\n",
      "\u001b[0m\n",
      "it 5/100, Jtr_pred = 0.734413, err = 0.483000, \u001b[31m   time: 65.572223 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.687071, err = 0.448333\n",
      "\u001b[0m\n",
      "it 6/100, Jtr_pred = 0.765169, err = 0.483500, \u001b[31m   time: 63.258970 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.695927, err = 0.460000\n",
      "\u001b[0m\n",
      "it 7/100, Jtr_pred = 0.805498, err = 0.497500, \u001b[31m   time: 58.169711 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.709224, err = 0.526667\n",
      "\u001b[0m\n",
      "it 8/100, Jtr_pred = 0.844465, err = 0.498500, \u001b[31m   time: 60.771249 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694664, err = 0.488333\n",
      "\u001b[0m\n",
      "it 9/100, Jtr_pred = 0.880592, err = 0.494000, \u001b[31m   time: 60.080410 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.703629, err = 0.528333\n",
      "\u001b[0m\n",
      "it 10/100, Jtr_pred = 0.914165, err = 0.486000, \u001b[31m   time: 58.116841 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 2/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.705796, err = 0.503333\n",
      "\u001b[0m\n",
      "it 11/100, Jtr_pred = 0.913336, err = 0.459500, \u001b[31m   time: 58.584453 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694007, err = 0.430000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 12/100, Jtr_pred = 0.911756, err = 0.461500, \u001b[31m   time: 57.578122 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.666054, err = 0.373333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 13/100, Jtr_pred = 0.958234, err = 0.478500, \u001b[31m   time: 57.012659 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.649605, err = 0.363333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 14/100, Jtr_pred = 0.952653, err = 0.452500, \u001b[31m   time: 56.462286 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.637462, err = 0.328333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 15/100, Jtr_pred = 0.976505, err = 0.448000, \u001b[31m   time: 58.446343 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.655253, err = 0.375000\n",
      "\u001b[0m\n",
      "it 16/100, Jtr_pred = 1.017759, err = 0.461500, \u001b[31m   time: 57.356388 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.648308, err = 0.380000\n",
      "\u001b[0m\n",
      "it 17/100, Jtr_pred = 1.108857, err = 0.469000, \u001b[31m   time: 57.787503 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.670957, err = 0.381667\n",
      "\u001b[0m\n",
      "it 18/100, Jtr_pred = 1.037435, err = 0.452500, \u001b[31m   time: 56.127318 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.658724, err = 0.366667\n",
      "\u001b[0m\n",
      "it 19/100, Jtr_pred = 1.061168, err = 0.457000, \u001b[31m   time: 56.969406 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.657312, err = 0.360000\n",
      "\u001b[0m\n",
      "it 20/100, Jtr_pred = 1.074579, err = 0.451500, \u001b[31m   time: 57.643867 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 3/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.627115, err = 0.345000\n",
      "\u001b[0m\n",
      "it 21/100, Jtr_pred = 1.063057, err = 0.448500, \u001b[31m   time: 59.071653 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.636794, err = 0.333333\n",
      "\u001b[0m\n",
      "it 22/100, Jtr_pred = 1.025628, err = 0.445500, \u001b[31m   time: 59.357136 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.609537, err = 0.293333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 23/100, Jtr_pred = 1.088438, err = 0.457000, \u001b[31m   time: 57.610857 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.636026, err = 0.343333\n",
      "\u001b[0m\n",
      "it 24/100, Jtr_pred = 1.123731, err = 0.449000, \u001b[31m   time: 58.260944 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.594240, err = 0.331667\n",
      "\u001b[0m\n",
      "it 25/100, Jtr_pred = 1.117394, err = 0.440500, \u001b[31m   time: 57.963296 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.617372, err = 0.313333\n",
      "\u001b[0m\n",
      "it 26/100, Jtr_pred = 1.131834, err = 0.434000, \u001b[31m   time: 59.107778 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.700634, err = 0.380000\n",
      "\u001b[0m\n",
      "it 27/100, Jtr_pred = 1.120783, err = 0.418500, \u001b[31m   time: 59.140427 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.605162, err = 0.303333\n",
      "\u001b[0m\n",
      "it 28/100, Jtr_pred = 1.181450, err = 0.423500, \u001b[31m   time: 57.249655 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.580733, err = 0.305000\n",
      "\u001b[0m\n",
      "it 29/100, Jtr_pred = 1.199924, err = 0.433000, \u001b[31m   time: 58.166259 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.581125, err = 0.308333\n",
      "\u001b[0m\n",
      "it 30/100, Jtr_pred = 1.118533, err = 0.433500, \u001b[31m   time: 57.618467 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 4/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.596188, err = 0.301667\n",
      "\u001b[0m\n",
      "it 31/100, Jtr_pred = 1.117339, err = 0.417000, \u001b[31m   time: 59.071474 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.605477, err = 0.311667\n",
      "\u001b[0m\n",
      "it 32/100, Jtr_pred = 1.204021, err = 0.435000, \u001b[31m   time: 58.585661 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.697594, err = 0.368333\n",
      "\u001b[0m\n",
      "it 33/100, Jtr_pred = 1.259763, err = 0.446500, \u001b[31m   time: 57.970825 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.663109, err = 0.346667\n",
      "\u001b[0m\n",
      "it 34/100, Jtr_pred = 1.220756, err = 0.418500, \u001b[31m   time: 58.745332 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.565622, err = 0.288333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 35/100, Jtr_pred = 1.231354, err = 0.427500, \u001b[31m   time: 59.785536 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.749636, err = 0.375000\n",
      "\u001b[0m\n",
      "it 36/100, Jtr_pred = 1.172392, err = 0.417000, \u001b[31m   time: 58.907237 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.765561, err = 0.400000\n",
      "\u001b[0m\n",
      "it 37/100, Jtr_pred = 1.214855, err = 0.426500, \u001b[31m   time: 59.278258 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.630058, err = 0.326667\n",
      "\u001b[0m\n",
      "it 38/100, Jtr_pred = 1.203602, err = 0.428500, \u001b[31m   time: 58.794963 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.664323, err = 0.350000\n",
      "\u001b[0m\n",
      "it 39/100, Jtr_pred = 1.193215, err = 0.427500, \u001b[31m   time: 58.344251 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.637654, err = 0.326667\n",
      "\u001b[0m\n",
      "it 40/100, Jtr_pred = 1.204609, err = 0.434000, \u001b[31m   time: 59.367815 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 5/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.580485, err = 0.296667\n",
      "\u001b[0m\n",
      "it 41/100, Jtr_pred = 1.213513, err = 0.453000, \u001b[31m   time: 61.652733 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.701159, err = 0.373333\n",
      "\u001b[0m\n",
      "it 42/100, Jtr_pred = 1.166182, err = 0.414500, \u001b[31m   time: 58.714770 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.620729, err = 0.308333\n",
      "\u001b[0m\n",
      "it 43/100, Jtr_pred = 1.100628, err = 0.404500, \u001b[31m   time: 58.326252 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.657840, err = 0.323333\n",
      "\u001b[0m\n",
      "it 44/100, Jtr_pred = 1.168395, err = 0.401500, \u001b[31m   time: 57.862699 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.628199, err = 0.310000\n",
      "\u001b[0m\n",
      "it 45/100, Jtr_pred = 1.213141, err = 0.412500, \u001b[31m   time: 57.799853 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.605337, err = 0.288333\n",
      "\u001b[0m\n",
      "it 46/100, Jtr_pred = 1.229705, err = 0.430500, \u001b[31m   time: 59.839498 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.658773, err = 0.318333\n",
      "\u001b[0m\n",
      "it 47/100, Jtr_pred = 1.193697, err = 0.417000, \u001b[31m   time: 58.873031 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.634495, err = 0.315000\n",
      "\u001b[0m\n",
      "it 48/100, Jtr_pred = 1.155731, err = 0.406000, \u001b[31m   time: 58.458400 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.628460, err = 0.306667\n",
      "\u001b[0m\n",
      "it 49/100, Jtr_pred = 1.163109, err = 0.397000, \u001b[31m   time: 58.512819 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.604610, err = 0.321667\n",
      "\u001b[0m\n",
      "it 50/100, Jtr_pred = 1.218395, err = 0.434000, \u001b[31m   time: 59.593239 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 6/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.641749, err = 0.313333\n",
      "\u001b[0m\n",
      "it 51/100, Jtr_pred = 1.117161, err = 0.393500, \u001b[31m   time: 59.417275 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.680874, err = 0.321667\n",
      "\u001b[0m\n",
      "it 52/100, Jtr_pred = 1.216690, err = 0.433000, \u001b[31m   time: 58.831262 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.744096, err = 0.360000\n",
      "\u001b[0m\n",
      "it 53/100, Jtr_pred = 1.153630, err = 0.436000, \u001b[31m   time: 58.617777 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690211, err = 0.361667\n",
      "\u001b[0m\n",
      "it 54/100, Jtr_pred = 1.171894, err = 0.412500, \u001b[31m   time: 59.004449 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.675279, err = 0.331667\n",
      "\u001b[0m\n",
      "it 55/100, Jtr_pred = 1.216206, err = 0.422500, \u001b[31m   time: 58.188592 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    Jdev = 0.665001, err = 0.333333\n",
      "\u001b[0m\n",
      "it 56/100, Jtr_pred = 1.101365, err = 0.410500, \u001b[31m   time: 59.251800 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.706459, err = 0.338333\n",
      "\u001b[0m\n",
      "it 57/100, Jtr_pred = 1.159966, err = 0.418500, \u001b[31m   time: 58.895545 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.689512, err = 0.353333\n",
      "\u001b[0m\n",
      "it 58/100, Jtr_pred = 1.145045, err = 0.410500, \u001b[31m   time: 62.473840 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.614188, err = 0.321667\n",
      "\u001b[0m\n",
      "it 59/100, Jtr_pred = 1.140649, err = 0.400500, \u001b[31m   time: 63.270737 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.590680, err = 0.261667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_SGHMC_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 60/100, Jtr_pred = 1.100580, err = 0.412000, \u001b[31m   time: 60.613827 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 7/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.645450, err = 0.306667\n",
      "\u001b[0m\n",
      "it 61/100, Jtr_pred = 1.116679, err = 0.417500, \u001b[31m   time: 59.465256 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.603248, err = 0.288333\n",
      "\u001b[0m\n",
      "it 62/100, Jtr_pred = 1.068832, err = 0.400500, \u001b[31m   time: 60.918658 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.627697, err = 0.303333\n",
      "\u001b[0m\n",
      "it 63/100, Jtr_pred = 1.089514, err = 0.390000, \u001b[31m   time: 61.059689 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.613858, err = 0.295000\n",
      "\u001b[0m\n",
      "it 64/100, Jtr_pred = 1.180901, err = 0.409500, \u001b[31m   time: 62.613432 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.606987, err = 0.265000\n",
      "\u001b[0m\n",
      "it 65/100, Jtr_pred = 1.151211, err = 0.417000, \u001b[31m   time: 64.350219 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.597912, err = 0.273333\n",
      "\u001b[0m\n",
      "it 66/100, Jtr_pred = 1.123813, err = 0.404000, \u001b[31m   time: 64.237295 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.620982, err = 0.286667\n",
      "\u001b[0m\n",
      "it 67/100, Jtr_pred = 1.122412, err = 0.402000, \u001b[31m   time: 64.727089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.585387, err = 0.271667\n",
      "\u001b[0m\n",
      "it 68/100, Jtr_pred = 1.132466, err = 0.418000, \u001b[31m   time: 64.915986 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.694549, err = 0.345000\n",
      "\u001b[0m\n",
      "it 69/100, Jtr_pred = 1.129369, err = 0.425500, \u001b[31m   time: 64.404310 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.709416, err = 0.356667\n",
      "\u001b[0m\n",
      "it 70/100, Jtr_pred = 1.084328, err = 0.434000, \u001b[31m   time: 63.789245 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 8/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.668917, err = 0.351667\n",
      "\u001b[0m\n",
      "it 71/100, Jtr_pred = 1.095877, err = 0.410000, \u001b[31m   time: 63.720829 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.647202, err = 0.321667\n",
      "\u001b[0m\n",
      "it 72/100, Jtr_pred = 1.144784, err = 0.408000, \u001b[31m   time: 64.420916 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.701030, err = 0.325000\n",
      "\u001b[0m\n",
      "it 73/100, Jtr_pred = 1.108691, err = 0.403000, \u001b[31m   time: 65.311056 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.674238, err = 0.311667\n",
      "\u001b[0m\n",
      "it 74/100, Jtr_pred = 1.092171, err = 0.414500, \u001b[31m   time: 65.237615 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.677160, err = 0.323333\n",
      "\u001b[0m\n",
      "it 75/100, Jtr_pred = 1.091073, err = 0.408000, \u001b[31m   time: 65.547585 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.657023, err = 0.338333\n",
      "\u001b[0m\n",
      "it 76/100, Jtr_pred = 1.034432, err = 0.390500, \u001b[31m   time: 64.019320 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.663156, err = 0.343333\n",
      "\u001b[0m\n",
      "it 77/100, Jtr_pred = 1.098083, err = 0.404500, \u001b[31m   time: 64.816434 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.802880, err = 0.406667\n",
      "\u001b[0m\n",
      "it 78/100, Jtr_pred = 1.083183, err = 0.417500, \u001b[31m   time: 64.671410 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.670147, err = 0.310000\n",
      "\u001b[0m\n",
      "it 79/100, Jtr_pred = 1.063583, err = 0.406000, \u001b[31m   time: 64.075048 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.614514, err = 0.316667\n",
      "\u001b[0m\n",
      "it 80/100, Jtr_pred = 1.117431, err = 0.443000, \u001b[31m   time: 67.140135 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 9/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.647194, err = 0.343333\n",
      "\u001b[0m\n",
      "it 81/100, Jtr_pred = 1.058646, err = 0.432500, \u001b[31m   time: 64.982763 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.658998, err = 0.306667\n",
      "\u001b[0m\n",
      "it 82/100, Jtr_pred = 1.048319, err = 0.408000, \u001b[31m   time: 65.807922 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.601074, err = 0.293333\n",
      "\u001b[0m\n",
      "it 83/100, Jtr_pred = 1.046229, err = 0.419000, \u001b[31m   time: 68.136063 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.637682, err = 0.296667\n",
      "\u001b[0m\n",
      "it 84/100, Jtr_pred = 1.134667, err = 0.430500, \u001b[31m   time: 67.717045 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.606971, err = 0.311667\n",
      "\u001b[0m\n",
      "it 85/100, Jtr_pred = 1.111077, err = 0.420000, \u001b[31m   time: 64.676378 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.597515, err = 0.300000\n",
      "\u001b[0m\n",
      "it 86/100, Jtr_pred = 1.157814, err = 0.414500, \u001b[31m   time: 66.710033 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.617509, err = 0.320000\n",
      "\u001b[0m\n",
      "it 87/100, Jtr_pred = 1.078737, err = 0.413000, \u001b[31m   time: 61.282013 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.648306, err = 0.335000\n",
      "\u001b[0m\n",
      "it 88/100, Jtr_pred = 0.995026, err = 0.399500, \u001b[31m   time: 68.296474 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.597990, err = 0.298333\n",
      "\u001b[0m\n",
      "it 89/100, Jtr_pred = 1.023145, err = 0.407000, \u001b[31m   time: 67.002746 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.586948, err = 0.283333\n",
      "\u001b[0m\n",
      "it 90/100, Jtr_pred = 0.984012, err = 0.391500, \u001b[31m   time: 67.645021 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 10/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.591801, err = 0.290000\n",
      "\u001b[0m\n",
      "it 91/100, Jtr_pred = 1.006164, err = 0.418000, \u001b[31m   time: 65.237762 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.611352, err = 0.306667\n",
      "\u001b[0m\n",
      "it 92/100, Jtr_pred = 0.936305, err = 0.386000, \u001b[31m   time: 66.965712 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.600875, err = 0.305000\n",
      "\u001b[0m\n",
      "it 93/100, Jtr_pred = 0.974744, err = 0.395500, \u001b[31m   time: 65.628608 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.605882, err = 0.301667\n",
      "\u001b[0m\n",
      "it 94/100, Jtr_pred = 1.109969, err = 0.400500, \u001b[31m   time: 64.101529 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.622222, err = 0.308333\n",
      "\u001b[0m\n",
      "it 95/100, Jtr_pred = 0.960695, err = 0.420000, \u001b[31m   time: 63.890368 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.636211, err = 0.336667\n",
      "\u001b[0m\n",
      "it 96/100, Jtr_pred = 0.937439, err = 0.389000, \u001b[31m   time: 64.937792 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.623359, err = 0.331667\n",
      "\u001b[0m\n",
      "it 97/100, Jtr_pred = 0.969127, err = 0.408500, \u001b[31m   time: 61.887654 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.652950, err = 0.343333\n",
      "\u001b[0m\n",
      "it 98/100, Jtr_pred = 1.012586, err = 0.413500, \u001b[31m   time: 61.864815 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.648282, err = 0.323333\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "    for x, y in trainloader:\n",
    "\n",
    "        if flat_ims:\n",
    "            x = x.view(x.shape[0], -1)\n",
    "\n",
    "        cost_pred, err = net.fit(x, y, burn_in=(i % re_burn < burn_in),\n",
    "                                 resample_momentum=(it_count % resample_its == 0),\n",
    "                                 resample_prior=(it_count % resample_prior_its == 0))\n",
    "        it_count += 1\n",
    "        err_train[i] += err\n",
    "        cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "    toc = time.time()\n",
    "\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "    net.update_lr(i)\n",
    "\n",
    "    # ---- save weights\n",
    "    if i % re_burn >= burn_in and i % sim_steps == 0:\n",
    "        net.save_sampled_net(max_samples=N_saves)\n",
    "    #net.save_sampled_net(max_samples=N_saves)\n",
    "    \n",
    "    if i % sim_steps == 0:\n",
    "        net.save_sampled_net(max_samples=N_saves)\n",
    "    \n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "            if flat_ims:\n",
    "                x = x.view(x.shape[0], -1)\n",
    "\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "            net.save(models_dir+'/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "## SAVE WEIGHTS\n",
    "net.save_weights(models_dir + '/state_dicts.pkl')\n",
    "\n",
    "#np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
    "#np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580a157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_object(net.weight_set_samples, models_dir+'/state_dicts.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7487d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE WEIGHTS\n",
    "#net.save_weights(models_dir + '/state_dicts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c46326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# fig cost vs its\n",
    "\n",
    "# fig cost vs its\n",
    "textsize = 15\n",
    "marker = 5\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(range(0, nb_epochs, nb_its_dev), np.clip(cost_dev[::nb_its_dev], a_min=-5, a_max=5), 'b-')\n",
    "ax1.plot(np.clip(cost_train, a_min=-5, a_max=5), 'r--')\n",
    "ax1.set_ylabel('Cross Entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "plt.title('classification costs')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/cost.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_ylabel('% error')\n",
    "ax2.plot(range(0, nb_epochs, nb_its_dev), err_dev[::nb_its_dev], 'b-')\n",
    "ax2.plot(err_train, 'r--')\n",
    "ax2.set_ylim(top=1, bottom=1e-3)\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/err.png', bbox_extra_artists=(lgd,), box_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b874b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e38f4d",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf03314",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174fdb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/test\", transform=transform_covid19)\n",
    "\n",
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "NTrainPoints = train_data_len\n",
    "\n",
    "\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "nb_its_dev = log_interval\n",
    "flat_ims = False\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                              num_workers=num_workers)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                              num_workers=num_workers)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "net = BNN_cat(channels_in = channels_in, side_in = image_trans_size, classes = classes, \n",
    "              N_train = NTrainPoints, lr = lr, cuda = use_cuda, grad_std_mul = grad_std_mul)\n",
    "\n",
    "\n",
    "\n",
    "with open(models_dir + '/state_dicts.pkl', 'rb') as input:\n",
    "    net.weight_set_samples = pickle.load(input)\n",
    "\n",
    "net.set_mode_train(False)\n",
    "print(models_dir)\n",
    "print('net', len(net.weight_set_samples))\n",
    "test_cost = 0  # Note that these are per sample\n",
    "test_err = 0\n",
    "\n",
    "test_predictions = np.zeros((test_data_len, classes))\n",
    "\n",
    "\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "    cost, err, probs = net.sample_eval(x, y, Nsamples, grad=False)  # , logits=True\n",
    "    \n",
    "    test_cost += cost\n",
    "    test_err += err.cpu().numpy()\n",
    "    test_predictions[nb_samples:nb_samples + len(x), :] = probs.numpy()\n",
    "    nb_samples += len(x)\n",
    "\n",
    "# test_cost /= nb_samples\n",
    "test_err /= nb_samples\n",
    "cprint('b', '    Loglike = %5.6f, err = %1.6f\\n' % (-test_cost, test_err))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c15cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b084d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)\n",
    "\n",
    "im_ind = np.random.randint(0, y_dev.shape[0])\n",
    "# im_ind = 90\n",
    "print(\"image number:\", im_ind)\n",
    "\n",
    "x, y = x_dev[im_ind], y_dev[im_ind]\n",
    "#x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "\n",
    "print(\"real number:\", y)\n",
    "\n",
    "#plt.imshow(ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "\n",
    "ims = []\n",
    "\n",
    "# ims.append(x_rot[:, :, :])\n",
    "ims.append(x)\n",
    "\n",
    "#ims = np.concatenate(ims)\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "y = np.ones(np.shape(ims)[0])*y\n",
    "#ims = np.expand_dims(ims, axis=1)\n",
    "ims = np.array(ims)\n",
    "\n",
    "cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples,\n",
    "                                   grad=False)  # , logits=True\n",
    "\n",
    "predictions = probs.numpy()\n",
    "\n",
    "print(\"predictions\", predictions)\n",
    "\n",
    "print(\"error\", err.cpu().numpy())\n",
    "\n",
    "# predictions.max(axis=1)[0]\n",
    "# selections = (predictions[:,i] == predictions.max(axis=1))\n",
    "print(\"predict\", predictions.argmax())\n",
    "\n",
    "print(im_ind)\n",
    "\n",
    "print(valset[im_ind][1])\n",
    "\n",
    "print(valset.class_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16274b3",
   "metadata": {},
   "source": [
    "# predict train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998eab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4880d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dev = []\n",
    "y_train_dev = []\n",
    "for x, y in trainloader:\n",
    "    x_train_dev.append(x.cpu().numpy())\n",
    "    y_train_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_train_dev = np.concatenate(x_train_dev)\n",
    "y_train_dev = np.concatenate(y_train_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "prob = []\n",
    "for i in range(0,train_data_len):\n",
    "    x, y = x_train_dev[i], y_train_dev[i]\n",
    "    \n",
    "    #x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "    #print(\"real number:\",y)\n",
    "    y_true.append(y)\n",
    "    #plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "    #plt.show()\n",
    "    ims=[]\n",
    "    #ims.append(x_rot[:,:,:])\n",
    "    ims.append(x)\n",
    "    #ims = np.concatenate(ims)\n",
    "    net.set_mode_train(False)\n",
    "    #y = np.ones(ims.shape[0])*y\n",
    "    y = np.ones(np.shape(ims)[0])*y\n",
    "    #print(y)\n",
    "    ims = np.array(ims)\n",
    "    #ims = np.expand_dims(ims, axis=1)\n",
    "    #print(y)\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples, grad=False) # , logits=True\n",
    "    predictions = probs.numpy()\n",
    "    prob.append(predictions)\n",
    "#     print(\"predictions\", predictions)\n",
    "#     print(\"error\", err.cpu().numpy())\n",
    "    y_pred.append(predictions.argmax())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "prob = np.array(prob)\n",
    "prob = prob.reshape(train_data_len, classes)\n",
    "\n",
    "if save_data == True:\n",
    "    save_path = 'SGHMC_predict_data'\n",
    "    mkdir(save_path)\n",
    "    file_name = \"SGHMC_train_epochs=%d_lr=%f_batch_size=%d_image_trans_size=%d.csv\" \\\n",
    "                % (nb_epochs, lr, batch_size, image_trans_size)\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    print('c', completeName)\n",
    "    if os.path.exists(completeName):\n",
    "        os.remove(completeName)\n",
    "    # df = pd.DataFrame(prob)\n",
    "    # df.to_csv(completeName)\n",
    "    np.savetxt(completeName, prob, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33f67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "100c0ac6",
   "metadata": {},
   "source": [
    "# predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce0203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824bb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "prob = []\n",
    "for i in range(0,test_data_len):\n",
    "    x, y = x_dev[i], y_dev[i]\n",
    "    #x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "    #print(\"real number:\",y)\n",
    "    y_true.append(y)\n",
    "    #plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "    #plt.show()\n",
    "    ims=[]\n",
    "    #ims.append(x_rot[:,:,:])\n",
    "    ims.append(x)\n",
    "    #ims = np.concatenate(ims)\n",
    "    net.set_mode_train(False)\n",
    "    #y = np.ones(ims.shape[0])*y\n",
    "    y = np.ones(np.shape(ims)[0])*y\n",
    "    \n",
    "    ims = np.array(ims)\n",
    "    #ims = np.expand_dims(ims, axis=1)\n",
    "    #print(y)\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples, grad=False) # , logits=True\n",
    "    predictions = probs.numpy()\n",
    "    prob.append(predictions)\n",
    "#     print(\"predictions\", predictions)\n",
    "#     print(\"error\", err.cpu().numpy())\n",
    "    y_pred.append(predictions.argmax())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "prob = np.array(prob)\n",
    "prob = prob.reshape(test_data_len, classes)\n",
    "\n",
    "if save_data == True:\n",
    "    save_path = 'SGHMC_predict_data'\n",
    "    mkdir(save_path)\n",
    "    file_name = \"SGHMC_epochs=%d_lr=%f_batch_size=%d_image_trans_size=%d.csv\" \\\n",
    "                % (nb_epochs, lr, batch_size, image_trans_size)\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    print('c', completeName)\n",
    "    if os.path.exists(completeName):\n",
    "        os.remove(completeName)\n",
    "    # df = pd.DataFrame(prob)\n",
    "    # df.to_csv(completeName)\n",
    "    np.savetxt(completeName, prob, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0f804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aca5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d9c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b288b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c14025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca22f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00d3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ac71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e20c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192bd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe635022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d0363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267fcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0da06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50a7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e3695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba611f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e5d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f41c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2048e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431d11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c5013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ad175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
