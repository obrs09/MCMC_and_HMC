{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7621e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd884f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import argparse\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndim\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import argparse\n",
    "import matplotlib\n",
    "from src.Stochastic_Gradient_Langevin_Dynamics.model import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import collections\n",
    "import h5py, sys\n",
    "import gzip\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4853ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_trans_size = 128\n",
    "prior_sig = 0.1\n",
    "batch_size = 40\n",
    "nb_epochs = 100\n",
    "models_dir =  'models_pSGLD_COVID150'\n",
    "# Where to save plots and error, accuracy vectors\n",
    "results_dir =  'results_pSGLD_COVID150'\n",
    "\n",
    "use_preconditioning =True\n",
    "lr = 0.00001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_data = True\n",
    "n_samples = batch_size\n",
    "sample_freq = 2\n",
    "burn_in = 20\n",
    "Nsamples = n_samples\n",
    "save_every = int(nb_epochs/20)  \n",
    "# We sample every 2 epochs as I have found samples to be correlated after only 1\n",
    "num_workers = 4\n",
    "nhid = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d2fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "\n",
    "\n",
    "def humansize(nbytes):\n",
    "    i = 0\n",
    "    while nbytes >= 1024 and i < len(suffixes) - 1:\n",
    "        nbytes /= 1024.\n",
    "        i += 1\n",
    "    f = ('%.2f' % nbytes)\n",
    "    return '%s%s' % (f, suffixes[i])\n",
    "\n",
    "\n",
    "def get_num_batches(nb_samples, batch_size, roundup=True):\n",
    "    if roundup:\n",
    "        return ((nb_samples + (-nb_samples % batch_size)) / batch_size)  # roundup division\n",
    "    else:\n",
    "        return nb_samples / batch_size\n",
    "\n",
    "\n",
    "def generate_ind_batch(nb_samples, batch_size, random=True, roundup=True):\n",
    "    if random:\n",
    "        ind = np.random.permutation(nb_samples)\n",
    "    else:\n",
    "        ind = range(int(nb_samples))\n",
    "    for i in range(int(get_num_batches(nb_samples, batch_size, roundup))):\n",
    "        yield ind[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def shuffle_in_unison_scary(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "class DatafeedImage(data.Dataset):\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "\n",
    "### functions for BNN with gauss output: ###\n",
    "\n",
    "def diagonal_gauss_loglike(x, mu, sigma):\n",
    "    # note that we can just treat each dim as isotropic and then do sum\n",
    "    cte_term = -(0.5)*np.log(2*np.pi)\n",
    "    det_sig_term = -torch.log(sigma)\n",
    "    inner = (x - mu)/sigma\n",
    "    dist_term = -(0.5)*(inner**2)\n",
    "    log_px = (cte_term + det_sig_term + dist_term).sum(dim=1, keepdim=False)\n",
    "    return log_px\n",
    "\n",
    "def get_rms(mu, y, y_means, y_stds):\n",
    "    x_un = mu * y_stds + y_means\n",
    "    y_un = y * y_stds + y_means\n",
    "    return torch.sqrt(((x_un - y_un)**2).sum() / y.shape[0])\n",
    "\n",
    "\n",
    "def get_loglike(mu, sigma, y, y_means, y_stds):\n",
    "    mu_un = mu * y_stds + y_means\n",
    "    y_un = y * y_stds + y_means\n",
    "    sigma_un = sigma * y_stds\n",
    "    ll = diagonal_gauss_loglike(y_un, mu_un, sigma_un)\n",
    "    return ll.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d57142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d5ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def isotropic_gauss_loglike(x, mu, sigma, do_sum=True):\n",
    "    cte_term = -(0.5) * np.log(2 * np.pi)\n",
    "    det_sig_term = -torch.log(sigma)\n",
    "    inner = (x - mu) / sigma\n",
    "    dist_term = -(0.5) * (inner ** 2)\n",
    "\n",
    "    if do_sum:\n",
    "        out = (cte_term + det_sig_term + dist_term).sum()  # sum over all weights\n",
    "    else:\n",
    "        out = (cte_term + det_sig_term + dist_term)\n",
    "    return out\n",
    "\n",
    "\n",
    "class laplace_prior(object):\n",
    "    def __init__(self, mu, b):\n",
    "        self.mu = mu\n",
    "        self.b = b\n",
    "\n",
    "    def loglike(self, x, do_sum=True):\n",
    "        if do_sum:\n",
    "            return (-np.log(2 * self.b) - torch.abs(x - self.mu) / self.b).sum()\n",
    "        else:\n",
    "            return (-np.log(2 * self.b) - torch.abs(x - self.mu) / self.b)\n",
    "\n",
    "\n",
    "class isotropic_gauss_prior(object):\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.cte_term = -(0.5) * np.log(2 * np.pi)\n",
    "        self.det_sig_term = -np.log(self.sigma)\n",
    "\n",
    "    def loglike(self, x, do_sum=True):\n",
    "\n",
    "        dist_term = -(0.5) * ((x - self.mu) / self.sigma) ** 2\n",
    "        if do_sum:\n",
    "            return (self.cte_term + self.det_sig_term + dist_term).sum()\n",
    "        else:\n",
    "            return (self.cte_term + self.det_sig_term + dist_term)\n",
    "\n",
    "\n",
    "class spike_slab_2GMM(object):\n",
    "    def __init__(self, mu1, mu2, sigma1, sigma2, pi):\n",
    "        self.N1 = isotropic_gauss_prior(mu1, sigma1)\n",
    "        self.N2 = isotropic_gauss_prior(mu2, sigma2)\n",
    "\n",
    "        self.pi1 = pi\n",
    "        self.pi2 = (1 - pi)\n",
    "\n",
    "    def loglike(self, x):\n",
    "        N1_ll = self.N1.loglike(x)\n",
    "        N2_ll = self.N2.loglike(x)\n",
    "\n",
    "        # Numerical stability trick -> unnormalising logprobs will underflow otherwise\n",
    "        max_loglike = torch.max(N1_ll, N2_ll)\n",
    "        normalised_like = self.pi1 * torch.exp(N1_ll - max_loglike) + self.pi2 * torch.exp(N2_ll - max_loglike)\n",
    "        loglike = torch.log(normalised_like) + max_loglike\n",
    "\n",
    "        return loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c3d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1eef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183d5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b78e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59161e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f389b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Linear_2L(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_hid):\n",
    "        super(Linear_2L, self).__init__()\n",
    "\n",
    "        self.n_hid = n_hid\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, self.n_hid)\n",
    "        self.fc2 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        self.fc3 = nn.Linear(self.n_hid, output_dim)\n",
    "\n",
    "        # choose your non linearity\n",
    "        # self.act = nn.Tanh()\n",
    "        # self.act = nn.Sigmoid()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        # self.act = nn.ELU(inplace=True)\n",
    "        # self.act = nn.SELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n",
    "        # -----------------\n",
    "        x = self.fc1(x)\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        x = self.fc2(x)\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        y = self.fc3(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Net_langevin(BaseNet):\n",
    "    eps = 1e-6\n",
    "\n",
    "    def __init__(self, lr=1e-3, channels_in=3, side_in=28, cuda=True, classes=10, N_train=60000, prior_sig=0,\n",
    "                 nhid=1200, use_p=False):\n",
    "        super(Net_langevin, self).__init__()\n",
    "        cprint('y', ' Creating Net!! ')\n",
    "        self.lr = lr\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.cuda = cuda\n",
    "        self.channels_in = channels_in\n",
    "        self.prior_sig = prior_sig\n",
    "        self.classes = classes\n",
    "        self.N_train = N_train\n",
    "        self.side_in = side_in\n",
    "        self.nhid = nhid\n",
    "        self.use_p = use_p\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.weight_set_samples = []\n",
    "        self.test = False\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(42)\n",
    "\n",
    "        self.model = Linear_2L(input_dim=self.channels_in * self.side_in * self.side_in, \n",
    "                               output_dim=self.classes, n_hid=self.nhid)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "        #             cudnn.benchmark = True\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "\n",
    "        if self.use_p:\n",
    "            self.optimizer = pSGLD(params=self.model.parameters(), \n",
    "                                   lr=self.lr, norm_sigma=self.prior_sig, addnoise=True)\n",
    "        else:\n",
    "            self.optimizer = SGLD(params=self.model.parameters(), \n",
    "                                  lr=self.lr, norm_sigma=self.prior_sig, addnoise=True)\n",
    "\n",
    "    #         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        out = self.model(x)\n",
    "        # We use mean because we treat the loss as an estimation of whole dataset's likelihood\n",
    "        loss = F.cross_entropy(out, y, reduction='mean')\n",
    "        loss = loss * self.N_train  # We scale the loss to represent the whole dataset\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data * x.shape[0] / self.N_train, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def save_sampled_net(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
    "\n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n",
    "\n",
    "        return None\n",
    "\n",
    "    def sample_eval(self, x, y, Nsamples=0, logits=True, train=False):\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        if logits:\n",
    "            mean_out = out.mean(dim=0, keepdim=False)\n",
    "            loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
    "            probs = F.softmax(mean_out, dim=1).data.cpu()\n",
    "\n",
    "        else:\n",
    "            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "            probs = mean_out.data.cpu()\n",
    "\n",
    "            log_mean_probs_out = torch.log(mean_out)\n",
    "            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
    "\n",
    "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def all_sample_eval(self, x, y, Nsamples):\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        prob_out = F.softmax(out, dim=2)\n",
    "        prob_out = prob_out.data\n",
    "\n",
    "        return prob_out\n",
    "\n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        weight_vec = []\n",
    "\n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "\n",
    "            for key in state_dict.keys():\n",
    "                if 'weight' in key:\n",
    "                    weight_mtx = state_dict[key].cpu()\n",
    "                    for weight in weight_mtx.view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "\n",
    "        return np.array(weight_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b5f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SGLD(Optimizer):\n",
    "    \"\"\"\n",
    "    SGLD optimiser based on pytorch's SGD.\n",
    "    Note that the weight decay is specified in terms of the gaussian prior sigma.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, norm_sigma=0, addnoise=True):\n",
    "\n",
    "        weight_decay = 1 / (norm_sigma ** 2)\n",
    "\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, addnoise=addnoise)\n",
    "\n",
    "        super(SGLD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                if group['addnoise']:\n",
    "\n",
    "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1) / np.sqrt(group['lr'])\n",
    "                    p.data.add_(-group['lr'],\n",
    "                                0.5 * d_p + langevin_noise)\n",
    "                else:\n",
    "                    p.data.add_(-group['lr'], 0.5 * d_p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class pSGLD(Optimizer):\n",
    "    \"\"\"\n",
    "    RMSprop preconditioned SGLD using pytorch rmsprop implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, norm_sigma=0, alpha=0.99, eps=1e-8, centered=False, addnoise=True):\n",
    "\n",
    "        weight_decay = 1 / (norm_sigma ** 2)\n",
    "\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, alpha=alpha, eps=eps, centered=centered, addnoise=addnoise)\n",
    "        super(pSGLD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(pSGLD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('centered', False)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            weight_decay = group['weight_decay']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "                    if group['centered']:\n",
    "                        state['grad_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "                state['step'] += 1\n",
    "\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                # sqavg x alpha + (1-alph) sqavg *(elemwise) sqavg\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, d_p, d_p)\n",
    "\n",
    "                if group['centered']:\n",
    "                    grad_avg = state['grad_avg']\n",
    "                    grad_avg.mul_(alpha).add_(1 - alpha, d_p)\n",
    "                    avg = square_avg.cmul(-1, grad_avg, grad_avg).sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                #                 print(avg.shape)\n",
    "                if group['addnoise']:\n",
    "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1) / np.sqrt(group['lr'])\n",
    "                    p.data.add_(-group['lr'],\n",
    "                                0.5 * d_p.div_(avg) + langevin_noise / torch.sqrt(avg))\n",
    "\n",
    "                else:\n",
    "                    p.data.addcdiv_(-group['lr'], 0.5 * d_p, avg)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0469f7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 21.11M\n",
      "pstr\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_29196/2671600186.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform_covid19 = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomRotation(degrees=(0, 180)),\n",
    "    transforms.Resize(image_trans_size),\n",
    "    transforms.CenterCrop(image_trans_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/test\", transform=transform_covid19)\n",
    "\n",
    "\n",
    "\n",
    "channels_in = trainset[0][0].size()[0]\n",
    "classes = np.shape(np.unique(trainset.targets))[0]\n",
    "\n",
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "NTrainPoints = train_data_len\n",
    "\n",
    "# # Where to save models weights\n",
    "# models_dir = args.models_dir\n",
    "# # Where to save plots and error, accuracy vectors\n",
    "# results_dir = args.results_dir\n",
    "\n",
    "if use_preconditioning == 1:\n",
    "    models_dir = models_dir\n",
    "    results_dir = results_dir\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "#\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "#\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "#\n",
    "# trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "# valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=num_workers)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=num_workers)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "if use_preconditioning == 1:\n",
    "    net = Net_langevin(lr=lr, channels_in=channels_in, side_in=image_trans_size, \n",
    "                       cuda=use_cuda, classes=classes, N_train=NTrainPoints,\n",
    "                       prior_sig=prior_sig, nhid=nhid, use_p=True)\n",
    "    pstr = 'p'\n",
    "    print('pstr')\n",
    "\n",
    "else:\n",
    "    net = Net_langevin(lr=lr, channels_in=channels_in, side_in=image_trans_size, \n",
    "                       cuda=use_cuda, classes=classes, N_train=NTrainPoints,\n",
    "                       prior_sig=prior_sig, nhid=nhid, use_p=False)\n",
    "\n",
    "#Net_langevin(lr=lr, channels_in=1, side_in=image_trans_size, cuda=use_cuda, classes=2, N_train=NTrainPoints, prior_sig=prior_sig)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "epoch = 0\n",
    "cprint('c', '\\nTrain:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef393c3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_29196/3299698679.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  d_p.add_(weight_decay, p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/100, Jtr_pred = 0.730354, err = 0.398500, \u001b[31m   time: 79.165018 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.645203, err = 0.341667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/100, Jtr_pred = 0.638154, err = 0.326000, \u001b[31m   time: 56.033425 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.622886, err = 0.325000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 2/100, Jtr_pred = 0.610842, err = 0.317000, \u001b[31m   time: 54.390169 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.593537, err = 0.313333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/100, Jtr_pred = 0.604018, err = 0.314000, \u001b[31m   time: 57.643000 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.605448, err = 0.311667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 4/100, Jtr_pred = 0.587232, err = 0.299500, \u001b[31m   time: 54.834815 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.595082, err = 0.286667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 5/100, Jtr_pred = 0.579546, err = 0.288000, \u001b[31m   time: 51.473103 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.577359, err = 0.295000\n",
      "\u001b[0m\n",
      "it 6/100, Jtr_pred = 0.571406, err = 0.289500, \u001b[31m   time: 54.356198 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.597835, err = 0.300000\n",
      "\u001b[0m\n",
      "it 7/100, Jtr_pred = 0.570911, err = 0.290000, \u001b[31m   time: 56.289862 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.629527, err = 0.316667\n",
      "\u001b[0m\n",
      "it 8/100, Jtr_pred = 0.572175, err = 0.283500, \u001b[31m   time: 54.279760 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.621197, err = 0.323333\n",
      "\u001b[0m\n",
      "it 9/100, Jtr_pred = 0.572068, err = 0.293500, \u001b[31m   time: 52.733409 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.624533, err = 0.311667\n",
      "\u001b[0m\n",
      "it 10/100, Jtr_pred = 0.559720, err = 0.276500, \u001b[31m   time: 54.253006 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.651408, err = 0.341667\n",
      "\u001b[0m\n",
      "it 11/100, Jtr_pred = 0.555872, err = 0.269500, \u001b[31m   time: 58.076609 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.633420, err = 0.300000\n",
      "\u001b[0m\n",
      "it 12/100, Jtr_pred = 0.549272, err = 0.269000, \u001b[31m   time: 55.878004 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.652955, err = 0.305000\n",
      "\u001b[0m\n",
      "it 13/100, Jtr_pred = 0.554431, err = 0.255000, \u001b[31m   time: 54.383049 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.693407, err = 0.318333\n",
      "\u001b[0m\n",
      "it 14/100, Jtr_pred = 0.555953, err = 0.261500, \u001b[31m   time: 55.392897 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.658834, err = 0.288333\n",
      "\u001b[0m\n",
      "it 15/100, Jtr_pred = 0.550541, err = 0.261000, \u001b[31m   time: 54.827215 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 1/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.640620, err = 0.278333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 16/100, Jtr_pred = 0.545999, err = 0.268500, \u001b[31m   time: 54.136764 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.697406, err = 0.303333\n",
      "\u001b[0m\n",
      "it 17/100, Jtr_pred = 0.531519, err = 0.257000, \u001b[31m   time: 53.145154 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.684978, err = 0.298333\n",
      "\u001b[0m\n",
      "it 18/100, Jtr_pred = 0.537671, err = 0.260500, \u001b[31m   time: 54.529901 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.690886, err = 0.315000\n",
      "\u001b[0m\n",
      "it 19/100, Jtr_pred = 0.537753, err = 0.265500, \u001b[31m   time: 53.346529 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.655585, err = 0.291667\n",
      "\u001b[0m\n",
      "it 20/100, Jtr_pred = 0.531778, err = 0.248500, \u001b[31m   time: 62.553668 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 2/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.640216, err = 0.283333\n",
      "\u001b[0m\n",
      "it 21/100, Jtr_pred = 0.528347, err = 0.257000, \u001b[31m   time: 54.904923 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.647871, err = 0.306667\n",
      "\u001b[0m\n",
      "it 22/100, Jtr_pred = 0.518693, err = 0.245500, \u001b[31m   time: 55.317124 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.668535, err = 0.291667\n",
      "\u001b[0m\n",
      "it 23/100, Jtr_pred = 0.522272, err = 0.248000, \u001b[31m   time: 55.407784 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.670356, err = 0.281667\n",
      "\u001b[0m\n",
      "it 24/100, Jtr_pred = 0.520332, err = 0.244000, \u001b[31m   time: 53.330524 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.708708, err = 0.313333\n",
      "\u001b[0m\n",
      "it 25/100, Jtr_pred = 0.513561, err = 0.246500, \u001b[31m   time: 52.793348 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 3/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.678721, err = 0.313333\n",
      "\u001b[0m\n",
      "it 26/100, Jtr_pred = 0.499877, err = 0.231500, \u001b[31m   time: 55.176126 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.651367, err = 0.295000\n",
      "\u001b[0m\n",
      "it 27/100, Jtr_pred = 0.501942, err = 0.236000, \u001b[31m   time: 62.213982 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.655375, err = 0.300000\n",
      "\u001b[0m\n",
      "it 28/100, Jtr_pred = 0.513687, err = 0.240000, \u001b[31m   time: 52.942622 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.646095, err = 0.300000\n",
      "\u001b[0m\n",
      "it 29/100, Jtr_pred = 0.504010, err = 0.238500, \u001b[31m   time: 56.369318 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.625140, err = 0.288333\n",
      "\u001b[0m\n",
      "it 30/100, Jtr_pred = 0.486683, err = 0.226000, \u001b[31m   time: 59.616047 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 4/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.641348, err = 0.281667\n",
      "\u001b[0m\n",
      "it 31/100, Jtr_pred = 0.484176, err = 0.235000, \u001b[31m   time: 53.163935 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.650144, err = 0.295000\n",
      "\u001b[0m\n",
      "it 32/100, Jtr_pred = 0.491820, err = 0.230500, \u001b[31m   time: 53.302247 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.652861, err = 0.281667\n",
      "\u001b[0m\n",
      "it 33/100, Jtr_pred = 0.488980, err = 0.224500, \u001b[31m   time: 59.623538 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.712892, err = 0.281667\n",
      "\u001b[0m\n",
      "it 34/100, Jtr_pred = 0.495183, err = 0.223000, \u001b[31m   time: 56.899437 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.687999, err = 0.280000\n",
      "\u001b[0m\n",
      "it 35/100, Jtr_pred = 0.488203, err = 0.231000, \u001b[31m   time: 54.572066 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 5/100\u001b[0m\n",
      "\u001b[32m    Jdev = 0.712571, err = 0.308333\n",
      "\u001b[0m\n",
      "it 36/100, Jtr_pred = 0.489579, err = 0.235500, \u001b[31m   time: 56.139009 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.733812, err = 0.311667\n",
      "\u001b[0m\n",
      "it 37/100, Jtr_pred = 0.481767, err = 0.228000, \u001b[31m   time: 54.521476 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.733936, err = 0.281667\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_29196/329136424.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mnb_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mcost_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_save = 15\n",
    "# save_every = 2  # We sample every 2 epochs as I have found samples to be correlated after only 1\n",
    "N_saves = 100  # Max number of saves\n",
    "###################################\n",
    "\n",
    "print('  init cost variables:')\n",
    "kl_cost_train = np.zeros(nb_epochs)\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_err = np.inf\n",
    "\n",
    "nb_its_dev = 1\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "\n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        cost_pred, err = net.fit(x, y)\n",
    "\n",
    "        err_train[i] += err\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "\n",
    "    # ---- save weights\n",
    "    if i >= start_save and i % save_every == 0:\n",
    "        net.save_sampled_net(max_samples=N_saves)\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "            net.save(models_dir+'/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "# Save weight samples from the posterior\n",
    "save_object(net.weight_set_samples, models_dir+'/state_dicts.pkl')\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# results\n",
    "cprint('c', '\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "## Save results for plots\n",
    "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
    "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# fig cost vs its\n",
    "\n",
    "textsize = 15\n",
    "marker = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=100)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], 'b-')\n",
    "ax1.plot(pred_cost_train, 'r--')\n",
    "ax1.set_ylabel('Cross Entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "plt.title('classification costs')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/cost.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_ylabel('% error')\n",
    "ax2.plot(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], 'b-')\n",
    "ax2.plot(100 * err_train, 'r--')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/err.png', bbox_extra_artists=(lgd,), box_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afbc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea0ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c351f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad80eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e5e7c0f",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/test\", transform=transform_covid19)\n",
    "\n",
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "NTrainPoints = train_data_len\n",
    "\n",
    "\n",
    "if use_preconditioning == 1:\n",
    "    print('ccccc')\n",
    "    models_dir = models_dir\n",
    "    results_dir = results_dir\n",
    "\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "print('models_dir', models_dir)\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=0)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=0)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "if use_preconditioning == 1:\n",
    "    net = Net_langevin(lr=lr, channels_in=channels_in, side_in=image_trans_size, \n",
    "                       cuda=use_cuda, classes=classes, N_train=NTrainPoints,\n",
    "                       prior_sig=prior_sig, nhid=nhid, use_p=True)\n",
    "    pstr = 'p'\n",
    "    print('predict pSGLD in')\n",
    "else:\n",
    "    net = Net_langevin(lr=lr, channels_in=channels_in, side_in=image_trans_size, \n",
    "                       cuda=use_cuda, classes=classes, N_train=NTrainPoints,\n",
    "                       prior_sig=prior_sig, nhid=nhid, use_p=False)\n",
    "    print(\"predict SGLD in\")\n",
    "\n",
    "if use_preconditioning == 1:\n",
    "    pstr = 'p'\n",
    "    with open(models_dir + '/state_dicts.pkl', 'rb') as input:\n",
    "        net.weight_set_samples = pickle.load(input)\n",
    "else:\n",
    "    with open(models_dir + '/state_dicts.pkl', 'rb') as input:\n",
    "        net.weight_set_samples = pickle.load(input)\n",
    "\n",
    "if use_cuda:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, \n",
    "                                            shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "\n",
    "else:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, \n",
    "                                            shuffle=False, pin_memory=False, num_workers=num_workers)\n",
    "test_cost = 0  # Note that these are per sample\n",
    "test_err = 0\n",
    "nb_samples = 0\n",
    "test_predictions = np.zeros((test_data_len, classes))\n",
    "\n",
    "\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "for j, (x, y) in enumerate(valloader):\n",
    "    cost, err, probs = net.sample_eval(x, y, Nsamples, logits=False) # , logits=True\n",
    "    #print('nettttttttttttttttttttttttttttttttttt', probs)\n",
    "    test_cost += cost\n",
    "    test_err += err.cpu().numpy()\n",
    "    test_predictions[nb_samples:nb_samples+len(x), :] = probs.numpy()\n",
    "    nb_samples += len(x)\n",
    "\n",
    "# test_cost /= nb_samples\n",
    "test_err /= nb_samples\n",
    "cprint('b', '    Loglike = %5.6f, err = %1.6f\\n' % (-test_cost, test_err))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "#print(x_dev.shape)\n",
    "#print(y_dev.shape)\n",
    "\n",
    "im_ind = np.random.randint(0, y_dev.shape[0])\n",
    "#im_ind = 90\n",
    "print(\"image number:\", im_ind)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "im_ind = np.random.randint(0, y_dev.shape[0])\n",
    "#im_ind = 90\n",
    "print(\"image number:\", im_ind)\n",
    "\n",
    "\n",
    "x, y = x_dev[im_ind], y_dev[im_ind]\n",
    "#x_rot = np.expand_dims(ndim.interpolation.rotate(x[:, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "\n",
    "print(\"real number:\", y)\n",
    "\n",
    "#plt.imshow(ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "\n",
    "\n",
    "ims=[]\n",
    "\n",
    "\n",
    "#ims.append(x_rot[:,:,:])\n",
    "ims.append(x)\n",
    "\n",
    "\n",
    "#ims = np.concatenate(ims)\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "y = np.ones(np.shape(ims)[0])*y\n",
    "#ims = np.expand_dims(ims, axis=1)\n",
    "ims = np.array(ims)\n",
    "\n",
    "cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), \n",
    "                                   Nsamples=Nsamples, logits=False) # , logits=True\n",
    "\n",
    "predictions = probs.numpy()\n",
    "\n",
    "#print(\"predictions\", predictions)\n",
    "\n",
    "print(\"error\", err.cpu().numpy())\n",
    "\n",
    "\n",
    "# predictions.max(axis=1)[0]\n",
    "# selections = (predictions[:,i] == predictions.max(axis=1))\n",
    "print(\"predict\", predictions.argmax())\n",
    "\n",
    "print(im_ind)\n",
    "\n",
    "#print(valset[im_ind][1])\n",
    "\n",
    "print(valset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860631a8",
   "metadata": {},
   "source": [
    "# predict train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dev = []\n",
    "y_train_dev = []\n",
    "for x, y in trainloader:\n",
    "    x_train_dev.append(x.cpu().numpy())\n",
    "    y_train_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_train_dev = np.concatenate(x_train_dev)\n",
    "y_train_dev = np.concatenate(y_train_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "prob = []\n",
    "for i in range(0,train_data_len):\n",
    "    x, y = x_train_dev[i], y_train_dev[i]\n",
    "    #x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "    #print(\"real number:\",y)\n",
    "    y_true.append(y)\n",
    "    #plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "    #plt.show()\n",
    "    ims=[]\n",
    "    #ims.append(x_rot[:,:,:])\n",
    "    ims.append(x)\n",
    "    #ims = np.concatenate(ims)\n",
    "    net.set_mode_train(False)\n",
    "    #y = np.ones(ims.shape[0])*y\n",
    "    y = np.ones(np.shape(ims)[0])*y\n",
    "    ims = np.array(ims)\n",
    "    #ims = np.expand_dims(ims, axis=1)\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples, logits=False) # , logits=True\n",
    "    predictions = probs.numpy()\n",
    "    prob.append(predictions)\n",
    "#     print(\"predictions\", predictions)\n",
    "#     print(\"error\", err.cpu().numpy())\n",
    "    y_pred.append(predictions.argmax())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "prob = np.array(prob)\n",
    "prob = prob.reshape(train_data_len, classes)\n",
    "\n",
    "if save_data == True:\n",
    "    save_path = 'pSGLD_predict_data'\n",
    "    if use_preconditioning == 1:\n",
    "        save_path = pstr + save_path\n",
    "        print('in pSGLD path', save_path)\n",
    "    mkdir(save_path)\n",
    "    file_name = \"pSGLD_train_epochs=%d_lr=%f_batch_size=%d_image_trans_size=%d.csv\" \\\n",
    "                % (nb_epochs, lr, batch_size, image_trans_size)\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    print('c', completeName)\n",
    "    if os.path.exists(completeName):\n",
    "        os.remove(completeName)\n",
    "    # df = pd.DataFrame(prob)\n",
    "    # df.to_csv(completeName)\n",
    "    np.savetxt(completeName, prob, delimiter=\",\")\n",
    "    # file1 = open(completeName, \"w\")\n",
    "    # for i in range(0, 41):\n",
    "    #\n",
    "    #     file1.write(str(prob[i]))\n",
    "    #     file1.write(\"\\n\")\n",
    "    # file1.close()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15839386",
   "metadata": {},
   "source": [
    "# predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d71ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "prob = []\n",
    "for i in range(0,test_data_len):\n",
    "    x, y = x_dev[i], y_dev[i]\n",
    "    #x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "    #print(\"real number:\",y)\n",
    "    y_true.append(y)\n",
    "    #plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "    #plt.show()\n",
    "    ims=[]\n",
    "    #ims.append(x_rot[:,:,:])\n",
    "    ims.append(x)\n",
    "    #ims = np.concatenate(ims)\n",
    "    net.set_mode_train(False)\n",
    "    #y = np.ones(ims.shape[0])*y\n",
    "    y = np.ones(np.shape(ims)[0])*y\n",
    "    ims = np.array(ims)\n",
    "    #ims = np.expand_dims(ims, axis=1)\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples, logits=False) # , logits=True\n",
    "    predictions = probs.numpy()\n",
    "    prob.append(predictions)\n",
    "#     print(\"predictions\", predictions)\n",
    "#     print(\"error\", err.cpu().numpy())\n",
    "    y_pred.append(predictions.argmax())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "prob = np.array(prob)\n",
    "prob = prob.reshape(test_data_len, classes)\n",
    "\n",
    "if save_data == True:\n",
    "    save_path = 'pSGLD_predict_data'\n",
    "    if use_preconditioning == 1:\n",
    "        save_path = pstr + save_path\n",
    "        print('in pSGLD path', save_path)\n",
    "    mkdir(save_path)\n",
    "    file_name = \"pSGLD_epochs=%d_lr=%f_batch_size=%d_image_trans_size=%d.csv\" \\\n",
    "                % (nb_epochs, lr, batch_size, image_trans_size)\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    print('c', completeName)\n",
    "    if os.path.exists(completeName):\n",
    "        os.remove(completeName)\n",
    "    # df = pd.DataFrame(prob)\n",
    "    # df.to_csv(completeName)\n",
    "    np.savetxt(completeName, prob, delimiter=\",\")\n",
    "    # file1 = open(completeName, \"w\")\n",
    "    # for i in range(0, 41):\n",
    "    #\n",
    "    #     file1.write(str(prob[i]))\n",
    "    #     file1.write(\"\\n\")\n",
    "    # file1.close()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d88e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca523d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5c414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4e7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
