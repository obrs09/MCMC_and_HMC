{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7621e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd884f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import argparse\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndim\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import argparse\n",
    "import matplotlib\n",
    "from src.Stochastic_Gradient_Langevin_Dynamics.model import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import collections\n",
    "import h5py, sys\n",
    "import gzip\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4853ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_trans_size = 128\n",
    "prior_sig = 0.1\n",
    "batch_size = 40\n",
    "nb_epochs = 100\n",
    "models_dir =  'models_pSGLD_COVID150'\n",
    "# Where to save plots and error, accuracy vectors\n",
    "results_dir =  'results_pSGLD_COVID150'\n",
    "\n",
    "use_preconditioning =True\n",
    "lr = 0.00005\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_data = True\n",
    "n_samples = batch_size\n",
    "sample_freq = 2\n",
    "burn_in = 1000\n",
    "Nsamples = n_samples\n",
    "save_every = int(nb_epochs/20)  \n",
    "# We sample every 2 epochs as I have found samples to be correlated after only 1\n",
    "num_workers = 4\n",
    "nhid = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d2fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as input:\n",
    "        return pickle.load(input)\n",
    "\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "\n",
    "\n",
    "def humansize(nbytes):\n",
    "    i = 0\n",
    "    while nbytes >= 1024 and i < len(suffixes) - 1:\n",
    "        nbytes /= 1024.\n",
    "        i += 1\n",
    "    f = ('%.2f' % nbytes)\n",
    "    return '%s%s' % (f, suffixes[i])\n",
    "\n",
    "\n",
    "def get_num_batches(nb_samples, batch_size, roundup=True):\n",
    "    if roundup:\n",
    "        return ((nb_samples + (-nb_samples % batch_size)) / batch_size)  # roundup division\n",
    "    else:\n",
    "        return nb_samples / batch_size\n",
    "\n",
    "\n",
    "def generate_ind_batch(nb_samples, batch_size, random=True, roundup=True):\n",
    "    if random:\n",
    "        ind = np.random.permutation(nb_samples)\n",
    "    else:\n",
    "        ind = range(int(nb_samples))\n",
    "    for i in range(int(get_num_batches(nb_samples, batch_size, roundup))):\n",
    "        yield ind[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def shuffle_in_unison_scary(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "class Datafeed(data.Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "class DatafeedImage(data.Dataset):\n",
    "    def __init__(self, x_train, y_train, transform=None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x_train[index]\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "\n",
    "### functions for BNN with gauss output: ###\n",
    "\n",
    "def diagonal_gauss_loglike(x, mu, sigma):\n",
    "    # note that we can just treat each dim as isotropic and then do sum\n",
    "    cte_term = -(0.5)*np.log(2*np.pi)\n",
    "    det_sig_term = -torch.log(sigma)\n",
    "    inner = (x - mu)/sigma\n",
    "    dist_term = -(0.5)*(inner**2)\n",
    "    log_px = (cte_term + det_sig_term + dist_term).sum(dim=1, keepdim=False)\n",
    "    return log_px\n",
    "\n",
    "def get_rms(mu, y, y_means, y_stds):\n",
    "    x_un = mu * y_stds + y_means\n",
    "    y_un = y * y_stds + y_means\n",
    "    return torch.sqrt(((x_un - y_un)**2).sum() / y.shape[0])\n",
    "\n",
    "\n",
    "def get_loglike(mu, sigma, y, y_means, y_stds):\n",
    "    mu_un = mu * y_stds + y_means\n",
    "    y_un = y * y_stds + y_means\n",
    "    sigma_un = sigma * y_stds\n",
    "    ll = diagonal_gauss_loglike(y_un, mu_un, sigma_un)\n",
    "    return ll.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d57142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d5ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def isotropic_gauss_loglike(x, mu, sigma, do_sum=True):\n",
    "    cte_term = -(0.5) * np.log(2 * np.pi)\n",
    "    det_sig_term = -torch.log(sigma)\n",
    "    inner = (x - mu) / sigma\n",
    "    dist_term = -(0.5) * (inner ** 2)\n",
    "\n",
    "    if do_sum:\n",
    "        out = (cte_term + det_sig_term + dist_term).sum()  # sum over all weights\n",
    "    else:\n",
    "        out = (cte_term + det_sig_term + dist_term)\n",
    "    return out\n",
    "\n",
    "\n",
    "class laplace_prior(object):\n",
    "    def __init__(self, mu, b):\n",
    "        self.mu = mu\n",
    "        self.b = b\n",
    "\n",
    "    def loglike(self, x, do_sum=True):\n",
    "        if do_sum:\n",
    "            return (-np.log(2 * self.b) - torch.abs(x - self.mu) / self.b).sum()\n",
    "        else:\n",
    "            return (-np.log(2 * self.b) - torch.abs(x - self.mu) / self.b)\n",
    "\n",
    "\n",
    "class isotropic_gauss_prior(object):\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.cte_term = -(0.5) * np.log(2 * np.pi)\n",
    "        self.det_sig_term = -np.log(self.sigma)\n",
    "\n",
    "    def loglike(self, x, do_sum=True):\n",
    "\n",
    "        dist_term = -(0.5) * ((x - self.mu) / self.sigma) ** 2\n",
    "        if do_sum:\n",
    "            return (self.cte_term + self.det_sig_term + dist_term).sum()\n",
    "        else:\n",
    "            return (self.cte_term + self.det_sig_term + dist_term)\n",
    "\n",
    "\n",
    "class spike_slab_2GMM(object):\n",
    "    def __init__(self, mu1, mu2, sigma1, sigma2, pi):\n",
    "        self.N1 = isotropic_gauss_prior(mu1, sigma1)\n",
    "        self.N2 = isotropic_gauss_prior(mu2, sigma2)\n",
    "\n",
    "        self.pi1 = pi\n",
    "        self.pi2 = (1 - pi)\n",
    "\n",
    "    def loglike(self, x):\n",
    "        N1_ll = self.N1.loglike(x)\n",
    "        N2_ll = self.N2.loglike(x)\n",
    "\n",
    "        # Numerical stability trick -> unnormalising logprobs will underflow otherwise\n",
    "        max_loglike = torch.max(N1_ll, N2_ll)\n",
    "        normalised_like = self.pi1 * torch.exp(N1_ll - max_loglike) + self.pi2 * torch.exp(N2_ll - max_loglike)\n",
    "        loglike = torch.log(normalised_like) + max_loglike\n",
    "\n",
    "        return loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c3d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1eef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183d5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b78e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59161e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f389b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Linear_2L(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_hid):\n",
    "        super(Linear_2L, self).__init__()\n",
    "\n",
    "        self.n_hid = n_hid\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, self.n_hid)\n",
    "        self.fc2 = nn.Linear(self.n_hid, self.n_hid)\n",
    "        self.fc3 = nn.Linear(self.n_hid, output_dim)\n",
    "\n",
    "        # choose your non linearity\n",
    "        # self.act = nn.Tanh()\n",
    "        # self.act = nn.Sigmoid()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        # self.act = nn.ELU(inplace=True)\n",
    "        # self.act = nn.SELU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n",
    "        # -----------------\n",
    "        x = self.fc1(x)\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        x = self.fc2(x)\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        y = self.fc3(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Net_langevin(BaseNet):\n",
    "    eps = 1e-6\n",
    "\n",
    "    def __init__(self, lr=1e-3, channels_in=3, side_in=28, cuda=True, classes=10, N_train=60000, prior_sig=0,\n",
    "                 nhid=1200, use_p=False):\n",
    "        super(Net_langevin, self).__init__()\n",
    "        cprint('y', ' Creating Net!! ')\n",
    "        self.lr = lr\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.cuda = cuda\n",
    "        self.channels_in = channels_in\n",
    "        self.prior_sig = prior_sig\n",
    "        self.classes = classes\n",
    "        self.N_train = N_train\n",
    "        self.side_in = side_in\n",
    "        self.nhid = nhid\n",
    "        self.use_p = use_p\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.weight_set_samples = []\n",
    "        self.test = False\n",
    "\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(42)\n",
    "\n",
    "        self.model = Linear_2L(input_dim=self.channels_in * self.side_in * self.side_in, \n",
    "                               output_dim=self.classes, n_hid=self.nhid)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "        #             cudnn.benchmark = True\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "\n",
    "        if self.use_p:\n",
    "            self.optimizer = pSGLD(params=self.model.parameters(), \n",
    "                                   lr=self.lr, norm_sigma=self.prior_sig, addnoise=True)\n",
    "        else:\n",
    "            self.optimizer = SGLD(params=self.model.parameters(), \n",
    "                                  lr=self.lr, norm_sigma=self.prior_sig, addnoise=True)\n",
    "\n",
    "    #         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        out = self.model(x)\n",
    "        # We use mean because we treat the loss as an estimation of whole dataset's likelihood\n",
    "        loss = F.cross_entropy(out, y, reduction='mean')\n",
    "        loss = loss * self.N_train  # We scale the loss to represent the whole dataset\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data * x.shape[0] / self.N_train, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def save_sampled_net(self, max_samples):\n",
    "\n",
    "        if len(self.weight_set_samples) >= max_samples:\n",
    "            self.weight_set_samples.pop(0)\n",
    "\n",
    "        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n",
    "\n",
    "        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n",
    "\n",
    "        return None\n",
    "\n",
    "    def sample_eval(self, x, y, Nsamples=0, logits=True, train=False):\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        if logits:\n",
    "            mean_out = out.mean(dim=0, keepdim=False)\n",
    "            loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
    "            probs = F.softmax(mean_out, dim=1).data.cpu()\n",
    "\n",
    "        else:\n",
    "            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "            probs = mean_out.data.cpu()\n",
    "\n",
    "            log_mean_probs_out = torch.log(mean_out)\n",
    "            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
    "\n",
    "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "\n",
    "    def all_sample_eval(self, x, y, Nsamples):\n",
    "        if Nsamples == 0:\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = x.data.new(Nsamples, x.shape[0], self.classes)\n",
    "\n",
    "        # iterate over all saved weight configuration samples\n",
    "        for idx, weight_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "            self.model.load_state_dict(weight_dict)\n",
    "            out[idx] = self.model(x)\n",
    "\n",
    "        prob_out = F.softmax(out, dim=2)\n",
    "        prob_out = prob_out.data\n",
    "\n",
    "        return prob_out\n",
    "\n",
    "    def get_weight_samples(self, Nsamples=0):\n",
    "        weight_vec = []\n",
    "\n",
    "        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n",
    "            Nsamples = len(self.weight_set_samples)\n",
    "\n",
    "        for idx, state_dict in enumerate(self.weight_set_samples):\n",
    "            if idx == Nsamples:\n",
    "                break\n",
    "\n",
    "            for key in state_dict.keys():\n",
    "                if 'weight' in key:\n",
    "                    weight_mtx = state_dict[key].cpu()\n",
    "                    for weight in weight_mtx.view(-1):\n",
    "                        weight_vec.append(weight)\n",
    "\n",
    "        return np.array(weight_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b5f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SGLD(Optimizer):\n",
    "    \"\"\"\n",
    "    SGLD optimiser based on pytorch's SGD.\n",
    "    Note that the weight decay is specified in terms of the gaussian prior sigma.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, norm_sigma=0, addnoise=True):\n",
    "\n",
    "        weight_decay = 1 / (norm_sigma ** 2)\n",
    "\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, addnoise=addnoise)\n",
    "\n",
    "        super(SGLD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                if group['addnoise']:\n",
    "\n",
    "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1) / np.sqrt(group['lr'])\n",
    "                    p.data.add_(-group['lr'],\n",
    "                                0.5 * d_p + langevin_noise)\n",
    "                else:\n",
    "                    p.data.add_(-group['lr'], 0.5 * d_p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class pSGLD(Optimizer):\n",
    "    \"\"\"\n",
    "    RMSprop preconditioned SGLD using pytorch rmsprop implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, norm_sigma=0, alpha=0.99, eps=1e-8, centered=False, addnoise=True):\n",
    "\n",
    "        weight_decay = 1 / (norm_sigma ** 2)\n",
    "\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, alpha=alpha, eps=eps, centered=centered, addnoise=addnoise)\n",
    "        super(pSGLD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(pSGLD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('centered', False)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            weight_decay = group['weight_decay']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "                    if group['centered']:\n",
    "                        state['grad_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "                state['step'] += 1\n",
    "\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "\n",
    "                # sqavg x alpha + (1-alph) sqavg *(elemwise) sqavg\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, d_p, d_p)\n",
    "\n",
    "                if group['centered']:\n",
    "                    grad_avg = state['grad_avg']\n",
    "                    grad_avg.mul_(alpha).add_(1 - alpha, d_p)\n",
    "                    avg = square_avg.cmul(-1, grad_avg, grad_avg).sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                #                 print(avg.shape)\n",
    "                if group['addnoise']:\n",
    "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1) / np.sqrt(group['lr'])\n",
    "                    p.data.add_(-group['lr'],\n",
    "                                0.5 * d_p.div_(avg) + langevin_noise / torch.sqrt(avg))\n",
    "\n",
    "                else:\n",
    "                    p.data.addcdiv_(-group['lr'], 0.5 * d_p, avg)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0469f7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 21.11M\n",
      "pstr\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_9900/2671600186.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(p.numel() for p in self.model.parameters())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform_covid19 = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomRotation(degrees=(0, 180)),\n",
    "    transforms.Resize(image_trans_size),\n",
    "    transforms.CenterCrop(image_trans_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/test\", transform=transform_covid19)\n",
    "\n",
    "\n",
    "\n",
    "channels_in = trainset[0][0].size()[0]\n",
    "classes = np.shape(np.unique(trainset.targets))[0]\n",
    "\n",
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "NTrainPoints = train_data_len\n",
    "\n",
    "# # Where to save models weights\n",
    "# models_dir = args.models_dir\n",
    "# # Where to save plots and error, accuracy vectors\n",
    "# results_dir = args.results_dir\n",
    "\n",
    "if use_preconditioning == 1:\n",
    "    models_dir = models_dir\n",
    "    results_dir = results_dir\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "#\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "#\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "#\n",
    "# trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "# valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=num_workers)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=num_workers)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "if use_preconditioning == 1:\n",
    "    net = Net_langevin(lr=lr, channels_in=channels_in, side_in=image_trans_size, \n",
    "                       cuda=use_cuda, classes=classes, N_train=NTrainPoints,\n",
    "                       prior_sig=prior_sig, nhid=nhid, use_p=True)\n",
    "    pstr = 'p'\n",
    "    print('pstr')\n",
    "\n",
    "else:\n",
    "    net = Net_langevin(lr=lr, channels_in=channels_in, side_in=image_trans_size, \n",
    "                       cuda=use_cuda, classes=classes, N_train=NTrainPoints,\n",
    "                       prior_sig=prior_sig, nhid=nhid, use_p=False)\n",
    "\n",
    "#Net_langevin(lr=lr, channels_in=1, side_in=image_trans_size, cuda=use_cuda, classes=2, N_train=NTrainPoints, prior_sig=prior_sig)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "epoch = 0\n",
    "cprint('c', '\\nTrain:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef393c3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHUAIZ~1\\AppData\\Local\\Temp/ipykernel_9900/3299698679.py:96: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  d_p.add_(weight_decay, p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/100, Jtr_pred = 2.563166, err = 0.400000, \u001b[31m   time: 58.302833 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.408957, err = 0.350000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/100, Jtr_pred = 1.350545, err = 0.325500, \u001b[31m   time: 57.095751 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.168693, err = 0.328333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 2/100, Jtr_pred = 1.174411, err = 0.313500, \u001b[31m   time: 58.176415 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.088053, err = 0.306667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/100, Jtr_pred = 0.922214, err = 0.282500, \u001b[31m   time: 57.562685 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.196684, err = 0.343333\n",
      "\u001b[0m\n",
      "it 4/100, Jtr_pred = 0.882167, err = 0.266500, \u001b[31m   time: 58.495484 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.146046, err = 0.351667\n",
      "\u001b[0m\n",
      "it 5/100, Jtr_pred = 0.837995, err = 0.267000, \u001b[31m   time: 69.387735 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.917279, err = 0.310000\n",
      "\u001b[0m\n",
      "it 6/100, Jtr_pred = 0.830104, err = 0.265000, \u001b[31m   time: 56.597637 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.010176, err = 0.315000\n",
      "\u001b[0m\n",
      "it 7/100, Jtr_pred = 0.740861, err = 0.250000, \u001b[31m   time: 58.805232 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.508065, err = 0.373333\n",
      "\u001b[0m\n",
      "it 8/100, Jtr_pred = 0.685174, err = 0.241500, \u001b[31m   time: 61.497566 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.026959, err = 0.306667\n",
      "\u001b[0m\n",
      "it 9/100, Jtr_pred = 0.700915, err = 0.242500, \u001b[31m   time: 76.694065 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.225039, err = 0.338333\n",
      "\u001b[0m\n",
      "it 10/100, Jtr_pred = 0.731263, err = 0.241500, \u001b[31m   time: 83.862780 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.179017, err = 0.345000\n",
      "\u001b[0m\n",
      "it 11/100, Jtr_pred = 0.668645, err = 0.218000, \u001b[31m   time: 84.348933 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.147266, err = 0.321667\n",
      "\u001b[0m\n",
      "it 12/100, Jtr_pred = 0.728489, err = 0.240000, \u001b[31m   time: 69.561734 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.408171, err = 0.328333\n",
      "\u001b[0m\n",
      "it 13/100, Jtr_pred = 0.723241, err = 0.233000, \u001b[31m   time: 62.677045 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.278459, err = 0.321667\n",
      "\u001b[0m\n",
      "it 14/100, Jtr_pred = 0.685541, err = 0.220500, \u001b[31m   time: 67.504318 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.457790, err = 0.363333\n",
      "\u001b[0m\n",
      "it 15/100, Jtr_pred = 0.758201, err = 0.235500, \u001b[31m   time: 67.507674 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 1/100\u001b[0m\n",
      "\u001b[32m    Jdev = 1.135470, err = 0.290000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 16/100, Jtr_pred = 0.687219, err = 0.231000, \u001b[31m   time: 68.868068 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.501911, err = 0.330000\n",
      "\u001b[0m\n",
      "it 17/100, Jtr_pred = 0.614547, err = 0.207000, \u001b[31m   time: 72.153701 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 2.112440, err = 0.391667\n",
      "\u001b[0m\n",
      "it 18/100, Jtr_pred = 0.636797, err = 0.216000, \u001b[31m   time: 70.541280 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.511407, err = 0.335000\n",
      "\u001b[0m\n",
      "it 19/100, Jtr_pred = 0.612500, err = 0.205500, \u001b[31m   time: 70.254243 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.204189, err = 0.300000\n",
      "\u001b[0m\n",
      "it 20/100, Jtr_pred = 0.608134, err = 0.199000, \u001b[31m   time: 68.979901 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 2/100\u001b[0m\n",
      "\u001b[32m    Jdev = 1.213238, err = 0.293333\n",
      "\u001b[0m\n",
      "it 21/100, Jtr_pred = 0.563136, err = 0.197000, \u001b[31m   time: 73.295987 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.445664, err = 0.343333\n",
      "\u001b[0m\n",
      "it 22/100, Jtr_pred = 0.610121, err = 0.206500, \u001b[31m   time: 71.496384 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.514376, err = 0.335000\n",
      "\u001b[0m\n",
      "it 23/100, Jtr_pred = 0.623182, err = 0.207500, \u001b[31m   time: 71.161393 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.317497, err = 0.296667\n",
      "\u001b[0m\n",
      "it 24/100, Jtr_pred = 0.610603, err = 0.191500, \u001b[31m   time: 70.053396 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.510219, err = 0.326667\n",
      "\u001b[0m\n",
      "it 25/100, Jtr_pred = 0.526418, err = 0.182000, \u001b[31m   time: 70.382873 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 3/100\u001b[0m\n",
      "\u001b[32m    Jdev = 1.248147, err = 0.308333\n",
      "\u001b[0m\n",
      "it 26/100, Jtr_pred = 0.539707, err = 0.186000, \u001b[31m   time: 72.538572 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.175499, err = 0.276667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 27/100, Jtr_pred = 0.538515, err = 0.177000, \u001b[31m   time: 57.653054 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.188825, err = 0.300000\n",
      "\u001b[0m\n",
      "it 28/100, Jtr_pred = 0.566806, err = 0.191500, \u001b[31m   time: 57.327555 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.092517, err = 0.273333\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 29/100, Jtr_pred = 0.610732, err = 0.197000, \u001b[31m   time: 58.299265 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.126518, err = 0.285000\n",
      "\u001b[0m\n",
      "it 30/100, Jtr_pred = 0.495193, err = 0.174000, \u001b[31m   time: 55.775838 seconds\n",
      "\u001b[0m\n",
      "\u001b[36m saving weight samples 4/100\u001b[0m\n",
      "\u001b[32m    Jdev = 1.069440, err = 0.276667\n",
      "\u001b[0m\n",
      "it 31/100, Jtr_pred = 0.526437, err = 0.179000, \u001b[31m   time: 55.751964 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.160472, err = 0.275000\n",
      "\u001b[0m\n",
      "it 32/100, Jtr_pred = 0.479319, err = 0.160500, \u001b[31m   time: 56.440278 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.141858, err = 0.256667\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting models_pSGLD_COVID150/theta_best.dat\n",
      "\u001b[0m\n",
      "it 33/100, Jtr_pred = 0.518958, err = 0.173500, \u001b[31m   time: 55.507947 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.331780, err = 0.306667\n",
      "\u001b[0m\n",
      "it 34/100, Jtr_pred = 0.573848, err = 0.179000, \u001b[31m   time: 58.294619 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.353799, err = 0.288333\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_save = 15\n",
    "# save_every = 2  # We sample every 2 epochs as I have found samples to be correlated after only 1\n",
    "N_saves = 100  # Max number of saves\n",
    "###################################\n",
    "\n",
    "print('  init cost variables:')\n",
    "kl_cost_train = np.zeros(nb_epochs)\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_err = np.inf\n",
    "\n",
    "nb_its_dev = 1\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "\n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        cost_pred, err = net.fit(x, y)\n",
    "\n",
    "        err_train[i] += err\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "\n",
    "    # ---- save weights\n",
    "    if i >= start_save and i % save_every == 0:\n",
    "        net.save_sampled_net(max_samples=N_saves)\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "            net.save(models_dir+'/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "# Save weight samples from the posterior\n",
    "save_object(net.weight_set_samples, models_dir+'/state_dicts.pkl')\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# results\n",
    "cprint('c', '\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "## Save results for plots\n",
    "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
    "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# fig cost vs its\n",
    "\n",
    "textsize = 15\n",
    "marker = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=100)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], 'b-')\n",
    "ax1.plot(pred_cost_train, 'r--')\n",
    "ax1.set_ylabel('Cross Entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "plt.title('classification costs')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/cost.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_ylabel('% error')\n",
    "ax2.plot(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], 'b-')\n",
    "ax2.plot(100 * err_train, 'r--')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(b=True, which='major', color='k', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='k', linestyle='--')\n",
    "ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "lgd = plt.legend(['test error', 'train error'], markerscale=marker, prop={'size': textsize, 'weight': 'normal'})\n",
    "ax = plt.gca()\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(textsize)\n",
    "    item.set_weight('normal')\n",
    "plt.savefig(results_dir + '/err.png', bbox_extra_artists=(lgd,), box_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afbc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea0ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c351f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad80eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e5e7c0f",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/train\", transform=transform_covid19)\n",
    "valset = torchvision.datasets.ImageFolder(root=\"./notebooks/data/COVID/test\", transform=transform_covid19)\n",
    "\n",
    "train_data_len = len(trainset.targets)\n",
    "test_data_len = len(valset.targets)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "NTrainPoints = train_data_len\n",
    "\n",
    "\n",
    "if use_preconditioning == 1:\n",
    "    print('ccccc')\n",
    "    models_dir = models_dir\n",
    "    results_dir = results_dir\n",
    "\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "print('models_dir', models_dir)\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=0)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=0)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "if use_preconditioning == 1:\n",
    "    net = Net_langevin(lr=lr, channels_in=channels_in, side_in=image_trans_size, \n",
    "                       cuda=use_cuda, classes=classes, N_train=NTrainPoints,\n",
    "                       prior_sig=prior_sig, nhid=nhid, use_p=True)\n",
    "    pstr = 'p'\n",
    "    print('predict pSGLD in')\n",
    "else:\n",
    "    net = Net_langevin(lr=lr, channels_in=channels_in, side_in=image_trans_size, \n",
    "                       cuda=use_cuda, classes=classes, N_train=NTrainPoints,\n",
    "                       prior_sig=prior_sig, nhid=nhid, use_p=False)\n",
    "    print(\"predict SGLD in\")\n",
    "\n",
    "if use_preconditioning == 1:\n",
    "    pstr = 'p'\n",
    "    with open(models_dir + '/state_dicts.pkl', 'rb') as input:\n",
    "        net.weight_set_samples = pickle.load(input)\n",
    "else:\n",
    "    with open(models_dir + '/state_dicts.pkl', 'rb') as input:\n",
    "        net.weight_set_samples = pickle.load(input)\n",
    "\n",
    "if use_cuda:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, \n",
    "                                            shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "\n",
    "else:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, \n",
    "                                            shuffle=False, pin_memory=False, num_workers=num_workers)\n",
    "test_cost = 0  # Note that these are per sample\n",
    "test_err = 0\n",
    "nb_samples = 0\n",
    "test_predictions = np.zeros((test_data_len, classes))\n",
    "\n",
    "\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "for j, (x, y) in enumerate(valloader):\n",
    "    cost, err, probs = net.sample_eval(x, y, Nsamples, logits=False) # , logits=True\n",
    "    #print('nettttttttttttttttttttttttttttttttttt', probs)\n",
    "    test_cost += cost\n",
    "    test_err += err.cpu().numpy()\n",
    "    test_predictions[nb_samples:nb_samples+len(x), :] = probs.numpy()\n",
    "    nb_samples += len(x)\n",
    "\n",
    "# test_cost /= nb_samples\n",
    "test_err /= nb_samples\n",
    "cprint('b', '    Loglike = %5.6f, err = %1.6f\\n' % (-test_cost, test_err))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = []\n",
    "y_dev = []\n",
    "for x, y in valloader:\n",
    "    x_dev.append(x.cpu().numpy())\n",
    "    y_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_dev = np.concatenate(x_dev)\n",
    "y_dev = np.concatenate(y_dev)\n",
    "#print(x_dev.shape)\n",
    "#print(y_dev.shape)\n",
    "\n",
    "im_ind = np.random.randint(0, y_dev.shape[0])\n",
    "#im_ind = 90\n",
    "print(\"image number:\", im_ind)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "im_ind = np.random.randint(0, y_dev.shape[0])\n",
    "#im_ind = 90\n",
    "print(\"image number:\", im_ind)\n",
    "\n",
    "\n",
    "x, y = x_dev[im_ind], y_dev[im_ind]\n",
    "#x_rot = np.expand_dims(ndim.interpolation.rotate(x[:, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "\n",
    "print(\"real number:\", y)\n",
    "\n",
    "#plt.imshow(ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "\n",
    "\n",
    "ims=[]\n",
    "\n",
    "\n",
    "#ims.append(x_rot[:,:,:])\n",
    "ims.append(x)\n",
    "\n",
    "\n",
    "#ims = np.concatenate(ims)\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "y = np.ones(np.shape(ims)[0])*y\n",
    "#ims = np.expand_dims(ims, axis=1)\n",
    "ims = np.array(ims)\n",
    "\n",
    "cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), \n",
    "                                   Nsamples=Nsamples, logits=False) # , logits=True\n",
    "\n",
    "predictions = probs.numpy()\n",
    "\n",
    "#print(\"predictions\", predictions)\n",
    "\n",
    "print(\"error\", err.cpu().numpy())\n",
    "\n",
    "\n",
    "# predictions.max(axis=1)[0]\n",
    "# selections = (predictions[:,i] == predictions.max(axis=1))\n",
    "print(\"predict\", predictions.argmax())\n",
    "\n",
    "print(im_ind)\n",
    "\n",
    "#print(valset[im_ind][1])\n",
    "\n",
    "print(valset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860631a8",
   "metadata": {},
   "source": [
    "# predict train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dev = []\n",
    "y_train_dev = []\n",
    "for x, y in trainloader:\n",
    "    x_train_dev.append(x.cpu().numpy())\n",
    "    y_train_dev.append(y.cpu().numpy())\n",
    "\n",
    "x_train_dev = np.concatenate(x_train_dev)\n",
    "y_train_dev = np.concatenate(y_train_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "prob = []\n",
    "for i in range(0,train_data_len):\n",
    "    x, y = x_train_dev[i], y_train_dev[i]\n",
    "    #x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "    #print(\"real number:\",y)\n",
    "    y_true.append(y)\n",
    "    #plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "    #plt.show()\n",
    "    ims=[]\n",
    "    #ims.append(x_rot[:,:,:])\n",
    "    ims.append(x)\n",
    "    #ims = np.concatenate(ims)\n",
    "    net.set_mode_train(False)\n",
    "    #y = np.ones(ims.shape[0])*y\n",
    "    y = np.ones(np.shape(ims)[0])*y\n",
    "    ims = np.array(ims)\n",
    "    #ims = np.expand_dims(ims, axis=1)\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples, logits=False) # , logits=True\n",
    "    predictions = probs.numpy()\n",
    "    prob.append(predictions)\n",
    "#     print(\"predictions\", predictions)\n",
    "#     print(\"error\", err.cpu().numpy())\n",
    "    y_pred.append(predictions.argmax())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "prob = np.array(prob)\n",
    "prob = prob.reshape(train_data_len, classes)\n",
    "\n",
    "if save_data == True:\n",
    "    save_path = 'SGLD_predict_data'\n",
    "    if use_preconditioning == 1:\n",
    "        save_path = pstr + save_path\n",
    "        print('in pSGLD path', save_path)\n",
    "    mkdir(save_path)\n",
    "    file_name = \"pSGLD_train_epochs=%d_lr=%f_batch_size=%d_image_trans_size=%d.csv\" \\\n",
    "                % (nb_epochs, lr, batch_size, image_trans_size)\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    print('c', completeName)\n",
    "    if os.path.exists(completeName):\n",
    "        os.remove(completeName)\n",
    "    # df = pd.DataFrame(prob)\n",
    "    # df.to_csv(completeName)\n",
    "    np.savetxt(completeName, prob, delimiter=\",\")\n",
    "    # file1 = open(completeName, \"w\")\n",
    "    # for i in range(0, 41):\n",
    "    #\n",
    "    #     file1.write(str(prob[i]))\n",
    "    #     file1.write(\"\\n\")\n",
    "    # file1.close()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15839386",
   "metadata": {},
   "source": [
    "# predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d71ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "prob = []\n",
    "for i in range(0,test_data_len):\n",
    "    x, y = x_dev[i], y_dev[i]\n",
    "    #x_rot = np.expand_dims(ndim.interpolation.rotate(x[0, :, :], 0, reshape=False, cval=-0.42421296), 0)\n",
    "    #print(\"real number:\",y)\n",
    "    y_true.append(y)\n",
    "    #plt.imshow( ndim.interpolation.rotate(x_dev[im_ind,0,:,:], 0, reshape=False))\n",
    "    #plt.show()\n",
    "    ims=[]\n",
    "    #ims.append(x_rot[:,:,:])\n",
    "    ims.append(x)\n",
    "    #ims = np.concatenate(ims)\n",
    "    net.set_mode_train(False)\n",
    "    #y = np.ones(ims.shape[0])*y\n",
    "    y = np.ones(np.shape(ims)[0])*y\n",
    "    ims = np.array(ims)\n",
    "    #ims = np.expand_dims(ims, axis=1)\n",
    "    cost, err, probs = net.sample_eval(torch.from_numpy(ims), torch.from_numpy(y), Nsamples=Nsamples, logits=False) # , logits=True\n",
    "    predictions = probs.numpy()\n",
    "    prob.append(predictions)\n",
    "#     print(\"predictions\", predictions)\n",
    "#     print(\"error\", err.cpu().numpy())\n",
    "    y_pred.append(predictions.argmax())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "prob = np.array(prob)\n",
    "prob = prob.reshape(test_data_len, classes)\n",
    "\n",
    "if save_data == True:\n",
    "    save_path = 'SGLD_predict_data'\n",
    "    if use_preconditioning == 1:\n",
    "        save_path = pstr + save_path\n",
    "        print('in pSGLD path', save_path)\n",
    "    mkdir(save_path)\n",
    "    file_name = \"pSGLD_epochs=%d_lr=%f_batch_size=%d_image_trans_size=%d.csv\" \\\n",
    "                % (nb_epochs, lr, batch_size, image_trans_size)\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    print('c', completeName)\n",
    "    if os.path.exists(completeName):\n",
    "        os.remove(completeName)\n",
    "    # df = pd.DataFrame(prob)\n",
    "    # df.to_csv(completeName)\n",
    "    np.savetxt(completeName, prob, delimiter=\",\")\n",
    "    # file1 = open(completeName, \"w\")\n",
    "    # for i in range(0, 41):\n",
    "    #\n",
    "    #     file1.write(str(prob[i]))\n",
    "    #     file1.write(\"\\n\")\n",
    "    # file1.close()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d88e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca523d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5c414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4e7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
